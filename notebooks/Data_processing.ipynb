{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cele_to_tcgm = pd.read_csv('/mnt/ionosphere-data/celestrak/SW-Last5Years.csv')\n",
    "\n",
    "print(cele_to_tcgm.head())\n",
    "\n",
    "#take the line in cele_to_tcgm that has in DATE 2024-05-10\n",
    "row = cele_to_tcgm[cele_to_tcgm['DATE'] == '2024-05-10']\n",
    "print(row)\n",
    "\n",
    "# I need this row to have as structure YYYY  DDD F10.7_OBS  F10.7_OBS_LAST81  Ap(daily)   Ap3h(0-3) Ap3h(3-6) Ap3h(6-9) Ap3h(9-12) Ap3h(12-15) Ap3h(15-18) Ap3h(18-21) Ap3h(21-24)\n",
    "# where DDD is the day of year\n",
    "from datetime import datetime\n",
    "date_str = row['DATE'].values[0]\n",
    "date_obj = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "year = date_obj.year\n",
    "day_of_year = date_obj.timetuple().tm_yday\n",
    "f107_obs = row['F10.7_OBS'].values[0]\n",
    "f107_obs_last81 = row['F10.7_OBS_LAST81'].values[0]\n",
    "ap_daily = row['AP_AVG'].values[0]\n",
    "ap3h_0_3 = row['AP1'].values[0]\n",
    "ap3h_3_6 = row['AP2'].values[0]\n",
    "ap3h_6_9 = row['AP3'].values[0]\n",
    "ap3h_9_12 = row['AP4'].values[0]\n",
    "ap3h_12_15 = row['AP5'].values[0]\n",
    "ap3h_15_18 = row['AP6'].values[0]\n",
    "ap3h_18_21 = row['AP7'].values[0]\n",
    "ap3h_21_24 = row['AP8'].values[0]\n",
    "# print in the required format\n",
    "\n",
    "output_line = f\"{year} {day_of_year} {f107_obs} {f107_obs_last81} {ap_daily} {ap3h_0_3} {ap3h_3_6} {ap3h_6_9} {ap3h_9_12} {ap3h_12_15} {ap3h_15_18} {ap3h_18_21} {ap3h_21_24}\"\n",
    "print(output_line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understand hot to process and make csv ready the data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "celestrak_data = pd.read_csv('/mnt/ionosphere-data/celestrak/SW-Last5Years.csv')\n",
    "celestrak_data_1 = pd.read_csv('/mnt/ionosphere-data/celestrak/kp_ap_timeseries.csv')\n",
    "\n",
    "#load line of 2020-01-01 from celestrak_data\n",
    "celestrak_data_2020 = celestrak_data[celestrak_data['DATE'] == '2022-01-01']\n",
    "#load line of 2020-01-01 from celestrak_data_1\n",
    "celestrak_data_2020_1 = celestrak_data_1[celestrak_data_1['Datetime'] == '2022-01-01 06:00:00']\n",
    "\n",
    "print(celestrak_data_2020)\n",
    "print(celestrak_data_2020_1)\n",
    "\n",
    "#Found an error the whole kp dataset is greater by a factor of 10. Correcting only the kp_ap_timeseries.csv file\n",
    "#celestrak_data_1['Kp'] = celestrak_data_1['Kp'] / 10\n",
    "#Save the corrected data\n",
    "\n",
    "print(celestrak_data_1.head()) # Do not touch pls\n",
    "\n",
    "#celestrak_data_1.to_csv('~/mnt/ionosphere-data/celestrak/kp_ap_timeseries.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "celestrak_data = pd.read_csv('/mnt/ionosphere-data/celestrak/kp_ap_timeseries.csv')\n",
    "\n",
    "print(celestrak_data)\n",
    "\n",
    "#plt a figure with two subplots histogram of the Kp and Ap values\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 8))\n",
    "axs[0].hist(celestrak_data['Kp'].dropna(), edgecolor='black', alpha=0.7)\n",
    "axs[0].set_title('Histogram of Kp Values')\n",
    "axs[0].set_xlabel('Kp Value')\n",
    "axs[0].set_yscale('log')  # Use logarithmic scale for better visibility\n",
    "axs[0].set_ylabel('Frequency')\n",
    "axs[1].hist(celestrak_data['Ap'].dropna(), bins = np.logspace(np.log10(1), np.log10(300), 20), edgecolor='black', alpha=0.7)\n",
    "axs[1].set_xscale('log')  # Use logarithmic scale for better visibility\n",
    "axs[1].set_yscale('log')  # Use logarithmic scale for better visibility\n",
    "axs[1].set_title('Histogram of Ap Values')\n",
    "axs[1].set_xlabel('Ap Value')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#plot the kp fpr the interval between the 8 of may from 12 of may of 2022\n",
    "\n",
    "time_start = '2022-05-08 00:00:00'\n",
    "time_end = '2022-05-12 23:59:59'\n",
    "\n",
    "mask = (celestrak_data['Datetime'] >= time_start) & (celestrak_data['Datetime'] <= time_end)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(pd.to_datetime(celestrak_data.loc[mask, 'Datetime']), celestrak_data.loc[mask, 'Kp'], marker='o')\n",
    "plt.title('Kp Index from 8 May to 12 May 2022')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Kp Index')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Remove for now\n",
    "\n",
    "# # find in the csv the position of the zero values for Kp and replace it with nan\n",
    "# celestrak_data['Kp'] = celestrak_data['Kp'].replace(0, np.nan)\n",
    "# celestrak_data['Ap'] = celestrak_data['Ap'].replace(0, np.nan)\n",
    "\n",
    "# #find how many nan there are in the csv and how many of them are consecutive\n",
    "# nan_count = celestrak_data.isna().sum()\n",
    "# print(\"Number of NaN values in each column:\")\n",
    "# print(nan_count)\n",
    "\n",
    "# nan_mask = celestrak_data['Kp'].isna()\n",
    "# celestrak_data['nan_group'] = (nan_mask != nan_mask.shift()).cumsum() * nan_mask\n",
    "# nan_group_sizes = celestrak_data[celestrak_data['nan_group'] > 0].groupby('nan_group').size()\n",
    "# nan_distribution = nan_group_sizes.value_counts().sort_index()\n",
    "# print(\"Distribution of consecutive NaN values:\")\n",
    "# print(nan_distribution)\n",
    "\n",
    "# #plot the distribution of consecutive NaN values\n",
    "\n",
    "# nan_distribution.plot(kind='bar')\n",
    "# plt.title('Distribution of Consecutive NaN Values in Kp')\n",
    "# plt.xlabel('Size of NaN Group')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.yscale('log')  # Use logarithmic scale for better visibility\n",
    "# plt.show()\n",
    "\n",
    "# #use linear interpolation to fill the NaN values in Kp and Ap\n",
    "# celestrak_data['Kp'] = celestrak_data['Kp'].interpolate(method='linear')\n",
    "# celestrak_data['Ap'] = celestrak_data['Ap'].interpolate(method='linear')\n",
    "# #save the processed data to a new csv file\n",
    "# #celestrak_data.to_csv('/mnt/ionosphere-data/celestrak/kp_ap_timeseries_processed.csv', index=False)\n",
    "\n",
    "# fig, axs = plt.subplots(2, 1, figsize=(10, 8))\n",
    "# axs[0].hist(celestrak_data['Kp'].dropna(), edgecolor='black', alpha=0.7)\n",
    "# axs[0].set_title('Histogram of Kp Values')\n",
    "# axs[0].set_xlabel('Kp Value')\n",
    "# axs[0].set_yscale('log')  # Use logarithmic scale for better visibility\n",
    "# axs[0].set_ylabel('Frequency')\n",
    "# #for the ap create bin spaced logarithmically\n",
    "# axs[1].hist(celestrak_data['Ap'].dropna(), \n",
    "#             bins = np.logspace(np.log10(1), np.log10(300), 20), edgecolor='black', alpha=0.7)\n",
    "# axs[1].set_xscale('log')  # Use logarithmic scale for better visibility\n",
    "# axs[1].set_yscale('log')  # Use logarithmic scale for better visibility\n",
    "# axs[1].set_title('Histogram of Ap Values')\n",
    "# axs[1].set_xlabel('Ap Value')\n",
    "# axs[1].set_ylabel('Frequency')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column in the celestrak_data dataframe with a label for the Kp value. \n",
    "\n",
    "# Where kp<5 put G0, when kp>5 put G1, when kp>6 put G2, when kp>7 put G3, when kp>8 put G4, when kp>9 put G5\n",
    "def label_kp(kp_value):\n",
    "    if kp_value < 5:\n",
    "        return 'G0'\n",
    "    elif kp_value < 6:\n",
    "        return 'G1'\n",
    "    elif kp_value < 7:\n",
    "        return 'G2'\n",
    "    elif kp_value < 8:\n",
    "        return 'G3'\n",
    "    elif kp_value < 9:\n",
    "        return 'G4'\n",
    "    else:\n",
    "        return 'G5'\n",
    "celestrak_data['Kp_Label'] = celestrak_data['Kp'].apply(label_kp)\n",
    "# Save the updated dataframe with Kp labels to a new CSV file\n",
    "\n",
    "print(celestrak_data.head())\n",
    "\n",
    "#celestrak_data.to_csv('/mnt/ionosphere-data/celestrak/kp_ap_timeseries_with_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the timeinterval longer than 9 hours where the average kp is greter than 5\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# # Detection function\n",
    "# def find_high_avg_kp_sliding_merged(data, window_hours=9, avg_threshold=6):\n",
    "#     data = data.copy()\n",
    "#     data['Datetime'] = pd.to_datetime(data['Datetime'])\n",
    "#     data = data.set_index('Datetime')\n",
    "\n",
    "#     time_step_hours = 3\n",
    "#     points_in_window = int(window_hours / time_step_hours)\n",
    "\n",
    "#     rolling = data['Kp'].rolling(window=points_in_window, min_periods=points_in_window).mean()\n",
    "#     valid_windows = rolling[rolling > avg_threshold]\n",
    "\n",
    "#     # Convert into DataFrame\n",
    "#     intervals = pd.DataFrame({\n",
    "#         'End': valid_windows.index,\n",
    "#         'Average_Kp': valid_windows.values\n",
    "#     })\n",
    "#     intervals['End'] = pd.to_datetime(intervals['End'])\n",
    "#     intervals['Start'] = intervals['End'] - pd.Timedelta(hours=window_hours)\n",
    "\n",
    "#     # Sort and merge overlapping intervals\n",
    "#     intervals = intervals.sort_values('Start').reset_index(drop=True)\n",
    "#     merged_intervals = []\n",
    "\n",
    "#     for _, row in intervals.iterrows():\n",
    "#         if not merged_intervals:\n",
    "#             merged_intervals.append(row)\n",
    "#         else:\n",
    "#             last = merged_intervals[-1]\n",
    "#             if row['Start'] <= last['End']:  # overlap\n",
    "#                 merged = pd.Series({\n",
    "#                     'Start': min(last['Start'], row['Start']),\n",
    "#                     'End': max(last['End'], row['End'])\n",
    "#                     })\n",
    "#             else:\n",
    "#                 merged_intervals.append(row)\n",
    "\n",
    "#     # Final recompute of stats for merged intervals\n",
    "#     final_results = []\n",
    "#     for interval in merged_intervals:\n",
    "#         start, end = interval['Start'], interval['End']\n",
    "#         segment = data[start:end]\n",
    "#         if len(segment) == 0:\n",
    "#             continue\n",
    "#         avg_kp = segment['Kp'].mean()\n",
    "#         max_kp = segment['Kp'].max()\n",
    "#         duration = (end - start).total_seconds() / 3600\n",
    "#         final_results.append({\n",
    "#             'Start': start,\n",
    "#             'End': end,\n",
    "#             'Duration_hours': duration,\n",
    "#             'Average_Kp': avg_kp,\n",
    "#             'Max_Kp': max_kp\n",
    "#         })\n",
    "\n",
    "#     return pd.DataFrame(final_results)\n",
    "\n",
    "\n",
    "# old function with threshold\n",
    "def find_strong_kp_intervals(data, initial_threshold=4.5, min_hours=9, avg_threshold=6):\n",
    "    data['Datetime'] = pd.to_datetime(data['Datetime'])\n",
    "    data = data.sort_values('Datetime').reset_index(drop=True)\n",
    "\n",
    "    # Step 1: Flag where Kp > initial threshold (e.g., 3)\n",
    "    data['above_initial'] = data['Kp'] > initial_threshold\n",
    "\n",
    "    # Step 2: Group consecutive values above threshold\n",
    "    data['group'] = (data['above_initial'] != data['above_initial'].shift()).cumsum()\n",
    "\n",
    "    results = []\n",
    "    for group_id, group_df in data.groupby('group'):\n",
    "        if not group_df['above_initial'].iloc[0]:\n",
    "            continue  # skip groups not above threshold\n",
    "\n",
    "        # Calculate duration\n",
    "        duration_hours = (group_df['Datetime'].iloc[-1] - group_df['Datetime'].iloc[0]).total_seconds() / 3600\n",
    "\n",
    "        if duration_hours >= min_hours:\n",
    "            avg_kp = group_df['Kp'].mean()\n",
    "            max_kp = group_df['Kp'].max()\n",
    "\n",
    "            if avg_threshold <= avg_kp < avg_threshold + 1: #avg_kp > avg_threshold condition to be inclusive\n",
    "            \n",
    "                results.append({\n",
    "                    'Start': group_df['Datetime'].iloc[0],\n",
    "                    'End': group_df['Datetime'].iloc[-1],\n",
    "                    'Duration_hours': duration_hours,\n",
    "                    'Average_Kp': avg_kp,\n",
    "                    'Max_Kp': max_kp\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "kp_intervals_sliding = find_strong_kp_intervals(celestrak_data, initial_threshold=4.5, min_hours=9, avg_threshold=6)\n",
    "kp_intervals_sliding = kp_intervals_sliding[kp_intervals_sliding['Start'] > '2010-01-01']\n",
    "print(\"High average Kp intervals after 2010:\")\n",
    "print(kp_intervals_sliding)\n",
    "print(len(kp_intervals_sliding), \"intervals found.\")\n",
    "\n",
    "\n",
    "# Output directory\n",
    "output_dir = '/mnt/ionosphere-data/events/'\n",
    "\n",
    "# Defined thresholds and windows\n",
    "thresholds = [5, 6, 7, 8, 9]               # 5 to 9 inclusive\n",
    "window_sizes = [3, 6, 9, 12]            # Only these 4 window lengths\n",
    "\n",
    "# Loop through combinations\n",
    "for avg_threshold in thresholds:\n",
    "    for window_hours in window_sizes:\n",
    "        kp_intervals_sliding = find_strong_kp_intervals(celestrak_data, initial_threshold=4.5, min_hours=window_hours, avg_threshold=avg_threshold)\n",
    "\n",
    "        # Filter to post-2010 events\n",
    "        df = kp_intervals_sliding[kp_intervals_sliding['Start'] > '2010-01-01']\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"No events for G{avg_threshold}H{window_hours}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Save as G{threshold}H{window}.csv\n",
    "        filename = f'G{avg_threshold-4}H{window_hours}.csv'\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "        df.to_csv(filepath, index=False)\n",
    "        print(f\"Saved {len(df)} events to {filename}\")\n",
    "\n",
    "\n",
    "\n",
    "#strong_kp_intervals = find_strong_kp_intervals(celestrak_data, initial_threshold=4, min_hours=9, avg_threshold=6)\n",
    "\n",
    "# print(\"Intervals where Kp > 4 and average Kp > 5 (duration > 9 hours):\")\n",
    "# print(strong_kp_intervals)\n",
    "\n",
    "#print values for start greatert than 2020-01-01\n",
    "\n",
    "# strong_kp_intervals['Start'] = pd.to_datetime(strong_kp_intervals['Start'])\n",
    "# strong_kp_intervals_filtered = strong_kp_intervals[strong_kp_intervals['Start'] > '2010-01-01']\n",
    "# print(\"Filtered intervals where Start > 2010-01-01:\")\n",
    "# print(strong_kp_intervals_filtered)\n",
    "\n",
    "# Save the filtered intervals to a CSV file\n",
    "\n",
    "#strong_kp_intervals_filtered.to_csv('/mnt/ionosphere-data/events/G2H9_extreme.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "data_kp_with_labels = pd.read_csv('/mnt/ionosphere-data/celestrak/kp_ap_timeseries_with_labels.csv')\n",
    "\n",
    "display(data_kp_with_labels)\n",
    "\n",
    "def classify_storm_events_by_label(df):\n",
    "    df = df.copy()\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "    df = df.sort_values('Datetime').reset_index(drop=True)\n",
    "\n",
    "    # Flag if this is a non-G0 label\n",
    "    df['is_storm'] = df['Kp_Label'] != 'G0'\n",
    "\n",
    "    # Group consecutive storm entries\n",
    "    df['group'] = (df['is_storm'] != df['is_storm'].shift()).cumsum()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for _, group_df in df.groupby('group'):\n",
    "        if not group_df['is_storm'].iloc[0]:\n",
    "            continue  # skip non-storm periods\n",
    "\n",
    "        start_time = group_df['Datetime'].iloc[0]\n",
    "        end_time = group_df['Datetime'].iloc[-1]\n",
    "        duration_hours = len(group_df) * 3  # Assuming 3-hour intervals\n",
    "\n",
    "        max_label = group_df['Kp_Label'].max()  # 'G1', 'G2', etc. (alphabetical but we can order it)\n",
    "        max_kp = group_df['Kp'].max()\n",
    "        average_kp = group_df['Kp'].mean()\n",
    "\n",
    "        results.append({\n",
    "            'Start': start_time,\n",
    "            'End': end_time,\n",
    "            'Duration_hours': duration_hours,\n",
    "            'Average_Kp': average_kp,\n",
    "            'Max_Kp': max_kp,\n",
    "            'Max_Kp_Label': max_label\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "#usage\n",
    "storm_intervals = classify_storm_events_by_label(data_kp_with_labels)\n",
    "# Filter to post-2010 events\n",
    "storm_events_df = storm_intervals[storm_intervals['Start'] > '2010-01-01']\n",
    "display(storm_events_df)\n",
    "\n",
    "# Output directory\n",
    "output_dir = '/mnt/ionosphere-data/events/'\n",
    "\n",
    "# Define storm levels and durations to categorize\n",
    "labels = ['G1', 'G2', 'G3', 'G4', 'G5']\n",
    "window_sizes = [3, 6, 9, 12]  # minimum durations (in hours)\n",
    "\n",
    "# Loop through combinations\n",
    "for label in labels:\n",
    "    for window_hours in window_sizes:\n",
    "        # Filter by label and duration\n",
    "        df = storm_events_df[\n",
    "            (storm_events_df['Max_Kp_Label'] == label) &\n",
    "            (storm_events_df['Duration_hours'] >= window_hours) &\n",
    "            (storm_events_df['Start'] > '2010-01-01')\n",
    "        ]\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"No events for {label}H{window_hours}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Save as G{label_number}H{window}.csv\n",
    "        label_number = int(label[1])  # e.g., \"G3\" → 3\n",
    "        filename = f'G{label_number}H{window_hours}_labels.csv'\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "        df.to_csv(filepath, index=False)\n",
    "        print(f\"Saved {len(df)} events to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "data_kp_with_labels = pd.read_csv('/mnt/ionosphere-data/celestrak/kp_ap_timeseries_with_labels.csv')\n",
    "\n",
    "def extract_quiet_periods(df, min_duration_hours_list=[3, 6, 9, 12]):\n",
    "    df = df.copy()\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "    df = df.sort_values('Datetime').reset_index(drop=True)\n",
    "\n",
    "    # Identify quiet periods (G0 only)\n",
    "    df['is_storm'] = df['Kp_Label'] != 'G0'\n",
    "    df['group'] = (df['is_storm'] != df['is_storm'].shift()).cumsum()\n",
    "\n",
    "    quiet_periods = []\n",
    "\n",
    "    for _, group_df in df.groupby('group'):\n",
    "        if group_df['is_storm'].iloc[0]:  # Skip storms, keep only quiet\n",
    "            continue\n",
    "\n",
    "        start = group_df['Datetime'].iloc[0]\n",
    "        end = group_df['Datetime'].iloc[-1]\n",
    "        duration_hours = len(group_df) * 3  # 3-hour cadence\n",
    "\n",
    "        quiet_periods.append({\n",
    "            'Start': start,\n",
    "            'End': end,\n",
    "            'Duration_hours': duration_hours,\n",
    "            'Max_Kp': group_df['Kp'].max(),\n",
    "            'Label': 'G0'\n",
    "        })\n",
    "\n",
    "    quiet_df = pd.DataFrame(quiet_periods)\n",
    "    display(quiet_df)\n",
    "\n",
    "    # Split by duration threshold and export\n",
    "    for window in min_duration_hours_list:\n",
    "        subset = quiet_df[quiet_df['Duration_hours'] >= window]\n",
    "        if subset.empty:\n",
    "            print(f\"No quiet periods ≥ {window}h found.\")\n",
    "            continue\n",
    "\n",
    "        filename = f'G0H{window}_labels_quiet.csv'\n",
    "        filepath = os.path.join('/mnt/ionosphere-data/events/', filename)\n",
    "        subset.to_csv(filepath, index=False)\n",
    "        print(f\"Saved {len(subset)} quiet events to {filename}\")\n",
    "\n",
    "    return quiet_df\n",
    "\n",
    "\n",
    "quiet_events = extract_quiet_periods(data_kp_with_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# --- FUNCTIONS ---\n",
    "\n",
    "def classify_duration_bin(duration, bins=[3, 6, 9, 12]):\n",
    "    \"\"\"Return the smallest bin >= duration.\"\"\"\n",
    "    for b in bins:\n",
    "        if duration >= b:\n",
    "            last_bin = b\n",
    "    return last_bin  # will end up with largest matching bin\n",
    "\n",
    "\n",
    "def extract_quiet_periods(df):\n",
    "    \"\"\"Extract continuous quiet periods (G0 only).\"\"\"\n",
    "    df = df.copy()\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "    df = df.sort_values('Datetime').reset_index(drop=True)\n",
    "\n",
    "    df['is_storm'] = df['Kp_Label'] != 'G0'\n",
    "    df['group'] = (df['is_storm'] != df['is_storm'].shift()).cumsum()\n",
    "\n",
    "    quiet_periods = []\n",
    "    for _, group_df in df.groupby('group'):\n",
    "        if group_df['is_storm'].iloc[0]:  # Skip storms\n",
    "            continue\n",
    "        duration_hours = len(group_df) * 3\n",
    "        duration_bin = classify_duration_bin(duration_hours)\n",
    "        quiet_periods.append({\n",
    "            'Start': group_df['Datetime'].iloc[0],\n",
    "            'End': group_df['Datetime'].iloc[-1],\n",
    "            'Duration_hours': duration_hours,\n",
    "            'Average_Kp': group_df['Kp'].mean(),\n",
    "            'Max_Kp': group_df['Kp'].max(),\n",
    "            'Max_Kp_Label': 'G0',\n",
    "            'Event_Index': f\"G0H{duration_bin}\"\n",
    "        })\n",
    "    return pd.DataFrame(quiet_periods)\n",
    "\n",
    "\n",
    "def classify_storm_events_by_label(df):\n",
    "    \"\"\"Extract continuous storm periods (non-G0).\"\"\"\n",
    "    df = df.copy()\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "    df = df.sort_values('Datetime').reset_index(drop=True)\n",
    "\n",
    "    df['is_storm'] = df['Kp_Label'] != 'G0'\n",
    "    df['group'] = (df['is_storm'] != df['is_storm'].shift()).cumsum()\n",
    "\n",
    "    results = []\n",
    "    for _, group_df in df.groupby('group'):\n",
    "        if not group_df['is_storm'].iloc[0]:\n",
    "            continue\n",
    "        duration_hours = len(group_df) * 3\n",
    "        duration_bin = classify_duration_bin(duration_hours)\n",
    "        results.append({\n",
    "            'Start': group_df['Datetime'].iloc[0],\n",
    "            'End': group_df['Datetime'].iloc[-1],\n",
    "            'Duration_hours': duration_hours,\n",
    "            'Average_Kp': group_df['Kp'].mean(),\n",
    "            'Max_Kp': group_df['Kp'].max(),\n",
    "            'Max_Kp_Label': group_df['Kp_Label'].max(),\n",
    "            'Event_Index': f\"{group_df['Kp_Label'].max()}H{duration_bin}\"\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# --- MAIN SCRIPT ---\n",
    "\n",
    "data_kp_with_labels = pd.read_csv('/mnt/ionosphere-data/celestrak/kp_ap_timeseries_with_labels.csv')\n",
    "\n",
    "# Extract quiet and storm events\n",
    "quiet_events = extract_quiet_periods(data_kp_with_labels)\n",
    "storm_events = classify_storm_events_by_label(data_kp_with_labels)\n",
    "\n",
    "# Keep only post-2010\n",
    "quiet_events = quiet_events[quiet_events['Start'] > '2010-01-01']\n",
    "storm_events = storm_events[storm_events['Start'] > '2010-01-01']\n",
    "\n",
    "output_dir = '/mnt/ionosphere-data/events/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "labels = ['G1', 'G2', 'G3', 'G4', 'G5']\n",
    "window_sizes = [3, 6, 9, 12]\n",
    "\n",
    "# --- 1. Save original per-label quiet events ---\n",
    "for window in window_sizes:\n",
    "    q_subset = quiet_events[quiet_events['Duration_hours'] >= window].copy()\n",
    "    if not q_subset.empty:\n",
    "        q_subset['Event_Index'] = 'G0H' + str(window)\n",
    "        filename = f'G0H{window}_labels_quiet.csv'\n",
    "        q_subset.to_csv(os.path.join(output_dir, filename), index=False)\n",
    "        print(f\"Saved {len(q_subset)} quiet events to {filename}\")\n",
    "\n",
    "# --- 2. Save original per-label storm events ---\n",
    "for label in labels:\n",
    "    for window in window_sizes:\n",
    "        s_subset = storm_events[\n",
    "            (storm_events['Max_Kp_Label'] == label) &\n",
    "            (storm_events['Duration_hours'] >= window)\n",
    "        ].copy()\n",
    "        if not s_subset.empty:\n",
    "            label_number = int(label[1])\n",
    "            s_subset['Event_Index'] = label + f\"H{window}\"\n",
    "            filename = f'G{label_number}H{window}_labels.csv'\n",
    "            s_subset.to_csv(os.path.join(output_dir, filename), index=False)\n",
    "            print(f\"Saved {len(s_subset)} storm events to {filename}\")\n",
    "\n",
    "# --- 3. Save merged events (quiet + storm) ---\n",
    "all_events = pd.concat([quiet_events, storm_events], ignore_index=True).sort_values('Start')\n",
    "for window in window_sizes:\n",
    "    e_subset = all_events[all_events['Duration_hours'] >= window].copy()\n",
    "    if not e_subset.empty:\n",
    "        e_subset['Event_Index'] = e_subset['Max_Kp_Label'] + f\"H{window}\"\n",
    "        filename = f'EventsH{window}_labels.csv'\n",
    "        e_subset.to_csv(os.path.join(output_dir, filename), index=False)\n",
    "        print(f\"Saved {len(e_subset)} total events to {filename}\")\n",
    "\n",
    "# --- 4. Save all events (all labels & durations) in one file ---\n",
    "# This merges all the above files into a single CSV without filtering again.\n",
    "\n",
    "all_event_files = []\n",
    "for f in os.listdir(output_dir):\n",
    "    if f.startswith('EventsH') and f.endswith('.csv') and f != 'Events_labels.csv':\n",
    "        df = pd.read_csv(os.path.join(output_dir, f))\n",
    "        all_event_files.append(df)\n",
    "\n",
    "if all_event_files:\n",
    "    all_events_combined = pd.concat(all_event_files, ignore_index=True).sort_values('Start')\n",
    "    all_events_combined.to_csv(os.path.join(output_dir, 'Events_labels.csv'), index=False)\n",
    "    print(f\"Saved {len(all_events_combined)} total combined events to Events_labels.csv\")\n",
    "else:\n",
    "    print(\"No event files found for final concatenation.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events = pd.concat([quiet_events, storm_events], ignore_index=True).sort_values('Start')\n",
    "all_events.to_csv(os.path.join(output_dir, 'Events_labels.csv'), index=False)\n",
    "print(f\"Saved {len(all_events)} total events to Events_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "indices_F10 = pd.read_csv('/mnt/ionosphere-data/solar_env_tech_indices/Indices_F10.csv')\n",
    "\n",
    "print(indices_F10)\n",
    "\n",
    "# #plot data distribution of F10.7 index\n",
    "# fig, axs = plt.subplots(2, 1, figsize=(10, 8))\n",
    "# axs[0].hist(indices_F10['F10'].dropna(), edgecolor='black', alpha=0.7)\n",
    "# axs[0].set_title('Histogram of F10.7 Index Values')\n",
    "# axs[0].set_xlabel('F10.7 Index Value')\n",
    "# axs[0].set_yscale('log')  # Use logarithmic scale for better visibility\n",
    "# axs[1].hist(indices_F10['S10'].dropna(), edgecolor='black', alpha=0.7)\n",
    "# axs[1].set_title('Histogram of S10 Index Values')\n",
    "# axs[1].set_xlabel('S10 Index Value')\n",
    "# axs[1].set_yscale('log')  # Use logarithmic scale for better visibility\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# print(np.where(indices_F10['S10'] == 0))  # Find the indices where F10.7 is zero\n",
    "\n",
    "\n",
    "# #count how many 0 are in the F10.7 index and replace them with nan\n",
    "# indices_F10['F10'] = indices_F10['F10'].replace(0, np.nan)\n",
    "# indices_F10['F81c'] = indices_F10['F10'].replace(0, np.nan)\n",
    "# indices_F10['S10'] = indices_F10['S10'].replace(0, np.nan)\n",
    "# indices_F10['S81c'] = indices_F10['S10'].replace(0, np.nan)\n",
    "# indices_F10['M10'] = indices_F10['S10'].replace(0, np.nan)\n",
    "# indices_F10['M81c'] = indices_F10['S10'].replace(0, np.nan)\n",
    "# indices_F10['Y10'] = indices_F10['S10'].replace(0, np.nan)\n",
    "# indices_F10['Y81c'] = indices_F10['S10'].replace(0, np.nan)\n",
    "\n",
    "# #find how many nan there are in the F10.7 index and how many of them are consecutive\n",
    "# nan_count_F10 = indices_F10.isna().sum()\n",
    "# print(\"Number of NaN values in F10.7 index:\")\n",
    "# print(nan_count_F10)\n",
    "# nan_mask_F10 = indices_F10['F10'].isna()\n",
    "# indices_F10['nan_group'] = (nan_mask_F10 != nan_mask_F10.shift()).cumsum() * nan_mask_F10\n",
    "# nan_group_sizes_F10 = indices_F10[indices_F10['nan_group'] > 0].groupby('nan_group').size()\n",
    "# nan_distribution_F10 = nan_group_sizes_F10.value_counts().sort_index()\n",
    "# print(\"Distribution of consecutive NaN values in F10.7 index:\")\n",
    "# print(nan_distribution_F10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "#take all the csv file in the fodler ~/mnt/ionosphere-data/omniweb/cleaned and concatenate them into one dataframe\n",
    "\n",
    "omni_files = [f for f in os.listdir('/mnt/ionosphere-data/omniweb/cleaned') if f.endswith('.csv')]\n",
    "omni_dataframes = []\n",
    "for file in omni_files:\n",
    "    file_path = os.path.join('/mnt/ionosphere-data/omniweb/cleaned', file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    omni_dataframes.append(df)\n",
    "# Concatenate all dataframes into one\n",
    "\n",
    "omni = pd.concat(omni_dataframes, ignore_index=True)\n",
    "\n",
    "print(omni.head())\n",
    "\n",
    "#check if datetime has any nan\n",
    "nan_count_datetime = omni['Datetime'].isna().sum()\n",
    "print(f\"Number of NaN values in 'datetime': {nan_count_datetime}\")\n",
    "\n",
    "# Check for NaN values in the GOES data\n",
    "\n",
    "nan_count_omni_goes = omni.isna().sum()\n",
    "print(\"Number of NaN values in Omni GOES data:\")\n",
    "print(nan_count_omni_goes)\n",
    "\n",
    "nan_mask_B = omni['B_mag'].isna()\n",
    "omni['nan_group_B'] = (nan_mask_B != nan_mask_B.shift()).cumsum() * nan_mask_B\n",
    "nan_group_sizes_B = omni[omni['nan_group_B'] > 0].groupby('nan_group_B').size()\n",
    "nan_distribution_B = nan_group_sizes_B.value_counts().sort_index()\n",
    "print(\"Distribution of consecutive NaN values in B_mag:\")\n",
    "print(nan_distribution_B)\n",
    "\n",
    "nan_mask_V = omni['V_flow'].isna()\n",
    "omni['nan_group_V'] = (nan_mask_V != nan_mask_V.shift()).cumsum() * nan_mask_V\n",
    "nan_group_sizes_V = omni[omni['nan_group_V'] > 0].groupby('nan_group_V').size()\n",
    "nan_distribution_V = nan_group_sizes_V.value_counts().sort_index()\n",
    "print(\"Distribution of consecutive NaN values in V_flow:\")\n",
    "print(nan_distribution_V)\n",
    "\n",
    "nan_mask_AE = omni['AE'].isna()\n",
    "omni['nan_group_AE'] = (nan_mask_AE != nan_mask_AE.shift()).cumsum() * nan_mask_AE\n",
    "nan_group_sizes_AE = omni[omni['nan_group_AE'] > 0].groupby('nan_group_AE').size()\n",
    "nan_distribution_AE = nan_group_sizes_AE.value_counts().sort_index()\n",
    "print(\"Distribution of consecutive NaN values in AE:\")\n",
    "print(nan_distribution_AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if the len of the nan_group is higher than 10, print starting and ending date from Datetime column\n",
    "\n",
    "omni['Datetime'] = pd.to_datetime(omni['Datetime'])\n",
    "\n",
    "def get_nan_gap_info(omni, column, group_col, min_size=11):\n",
    "    \"\"\"Returns a DataFrame with NaN gap start, end, and duration (in minutes)\"\"\"\n",
    "    nan_groups = omni[omni[group_col] > 0].groupby(group_col)\n",
    "    gap_info = []\n",
    "\n",
    "    for _, group_df in nan_groups:\n",
    "        size = len(group_df)\n",
    "        if size > min_size:\n",
    "            start = group_df['Datetime'].iloc[0]\n",
    "            end = group_df['Datetime'].iloc[-1]\n",
    "            duration_minutes = (end - start).total_seconds() / 60\n",
    "            gap_info.append((start, end, duration_minutes))\n",
    "\n",
    "    # Create DataFrame and sort\n",
    "    gap_df = pd.DataFrame(gap_info, columns=['Start', 'End', 'Duration_minutes'])\n",
    "    gap_df = gap_df.sort_values(by=['Duration_minutes', 'Start'], ascending=[True, True])\n",
    "    return gap_df\n",
    "\n",
    "# Generate DataFrames\n",
    "bmag_gaps = get_nan_gap_info(omni, 'B_mag', 'nan_group_B')\n",
    "vflow_gaps = get_nan_gap_info(omni, 'V_flow', 'nan_group_V')\n",
    "ae_gaps = get_nan_gap_info(omni, 'AE', 'nan_group_AE')\n",
    "\n",
    "display(bmag_gaps)\n",
    "display(vflow_gaps)\n",
    "display(ae_gaps)\n",
    "\n",
    "# Save to CSV files\n",
    "# bmag_gaps.to_csv('/mnt/ionosphere-data/events/bmag_nan_gaps.csv', index=False)\n",
    "# vflow_gaps.to_csv('/mnt/ionosphere-data/events/vflow_nan_gaps.csv', index=False)\n",
    "# ae_gaps.to_csv('/mnt/ionosphere-data/events/ae_nan_gaps.csv', index=False)\n",
    "\n",
    "\n",
    "'''\n",
    "def print_sorted_nan_gaps_by_duration(omni, column, group_col):\n",
    "    print(f\"\\n--- {column} NaN Gaps Sorted by Duration (asc) then Start Time (asc) ---\")\n",
    "\n",
    "    nan_groups = omni[omni[group_col] > 0].groupby(group_col)\n",
    "    gap_info = []\n",
    "\n",
    "    for _, group_df in nan_groups:\n",
    "        size = len(group_df)\n",
    "        if size > 12:  # Adjust threshold if needed\n",
    "            start = group_df['Datetime'].iloc[0]\n",
    "            end = group_df['Datetime'].iloc[-1]\n",
    "            duration_minutes = (end - start).total_seconds() / 60\n",
    "            gap_info.append((start, end, duration_minutes))\n",
    "\n",
    "    # Sort by duration ascending, then start time\n",
    "    gap_info_sorted = sorted(gap_info, key=lambda x: (x[2], x[0]))\n",
    "\n",
    "    # Print results\n",
    "    for start, end, duration in gap_info_sorted:\n",
    "        print(f\"[{column}] Start: {start}, End: {end}, Duration: {duration:.1f} minutes\")\n",
    "\n",
    "# Run for each target variable\n",
    "print_sorted_nan_gaps_by_duration(omni, 'B_mag', 'nan_group_B')\n",
    "print_sorted_nan_gaps_by_duration(omni, 'V_flow', 'nan_group_V')\n",
    "print_sorted_nan_gaps_by_duration(omni, 'AE', 'nan_group_AE')\n",
    "'''\n",
    "# for group_id, group_df in omni.groupby('nan_group_B'):\n",
    "#     if group_id == 0:\n",
    "#         continue\n",
    "#     if len(group_df) > 10:\n",
    "#         start_date = group_df['Datetime'].iloc[0]\n",
    "#         end_date = group_df['Datetime'].iloc[-1]\n",
    "#         print(f\"[B_mag] Group ID: {group_id}, Size: {len(group_df)}, Start: {start_date}, End: {end_date}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#create image of outputs for the days between the 10 and 14 of may 2024\n",
    "\n",
    "time_start = '2024-05-10 00:00:00'\n",
    "time_end = '2024-05-11 23:59:59'\n",
    "\n",
    "indices_F10 = pd.read_csv('/mnt/ionosphere-data/solar_env_tech_indices/Indices_F10_processed.csv')\n",
    "kp_ap_timeseries = pd.read_csv('/mnt/ionosphere-data/celestrak/kp_ap_processed_timeseries.csv')\n",
    "omni = pd.read_csv('/mnt/ionosphere-data/omniweb/csv/omni_5min_full_cleaned.csv')\n",
    "\n",
    "print(indices_F10.head())\n",
    "print(kp_ap_timeseries.head())\n",
    "print(omni.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indices_F10['Datetime'] = pd.to_datetime(indices_F10['Datetime'])\n",
    "kp_ap_timeseries['Datetime'] = pd.to_datetime(kp_ap_timeseries['Datetime'])\n",
    "omni['Datetime'] = pd.to_datetime(omni['Datetime'])\n",
    "\n",
    "#filter the dataframes for the time interval\n",
    "indices_F10_filtered = indices_F10[(indices_F10['Datetime'] >= time_start) & (indices_F10['Datetime'] <= time_end)]\n",
    "kp_ap_timeseries_filtered = kp_ap_timeseries[(kp_ap_timeseries['Datetime'] >= time_start) & (kp_ap_timeseries['Datetime'] <= time_end)]\n",
    "omni_filtered = omni[(omni['Datetime'] >= time_start) & (omni['Datetime'] <= time_end)]\n",
    "\n",
    "#plot in a single figure with 5 subplots the 1) F10, 2) Bz_GSE, Bx_GSE, By_GSE, 3) V_flow, 4) Kp, 5) AE \n",
    "\n",
    "fig, axs = plt.subplots(5, 1, figsize=(8, 14), sharex=True)\n",
    "# 1) F10\n",
    "axs[0].plot(indices_F10_filtered['Datetime'], indices_F10_filtered['F10'], color='orange', label='F10.7')\n",
    "axs[0].set_ylabel('F10.7 (sfu)', fontsize =14)\n",
    "axs[0].set_title('Solar Irradiance proxy F10.7', fontsize = 16)\n",
    "axs[0].grid()\n",
    "#axs[0].legend()\n",
    "# 2) Bz_GSE, Bx_GSE, By_GSE\n",
    "axs[1].plot(omni_filtered['Datetime'], omni_filtered['Bz_GSE'], color='red', label='Bz_GSE')\n",
    "axs[1].plot(omni_filtered['Datetime'], omni_filtered['Bx_GSE'], color='green', label='Bx_GSE')\n",
    "axs[1].plot(omni_filtered['Datetime'], omni_filtered['By_GSE'], color='blue', label='By_GSE')\n",
    "axs[1].set_ylabel('B$_{i}$ (nT)', fontsize =14)\n",
    "axs[1].set_title('Solar Wind Magnetic Field Components', fontsize = 16)\n",
    "axs[1].grid()\n",
    "axs[1].legend()\n",
    "# 3) V_flow\n",
    "axs[2].plot(omni_filtered['Datetime'], omni_filtered['V_flow'], color='purple', label='V_flow')\n",
    "axs[2].set_ylabel('V_flow (km/s)', fontsize =14)\n",
    "axs[2].set_title('Solar Wind Speed', fontsize = 16)\n",
    "axs[2].grid()\n",
    "#axs[2].legend()\n",
    "# 4) Kp\n",
    "axs[3].plot(kp_ap_timeseries_filtered['Datetime'], kp_ap_timeseries_filtered['Kp'], color='black', label='Kp Index')\n",
    "axs[3].set_ylabel('Kp Index', fontsize =14)\n",
    "axs[3].set_title('Geomagnetic Activity Index Kp', fontsize = 16)\n",
    "axs[3].grid()\n",
    "#axs[3].legend()\n",
    "# 5) AE\n",
    "axs[4].plot(omni_filtered['Datetime'], omni_filtered['AE'], color='brown', label='AE Index')\n",
    "axs[4].set_ylabel('AE Index (nT)', fontsize =14)\n",
    "axs[4].set_title('Auroral Electrojet (AE) Index', fontsize = 16)\n",
    "axs[4].grid()\n",
    "#axs[4].legend()\n",
    "axs[4].set_xlim([pd.to_datetime(time_start), pd.to_datetime(time_end)])\n",
    "\n",
    "for ax in axs:\n",
    "    ax.tick_params(axis='both', labelsize=12)  # Adjust tick label font size\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.xlabel('Date (2024)', fontsize = 14)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for supermag \n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data_sme = pd.read_csv('/mnt/ionosphere-data/supermag_auroral_index/supermag_SME.csv')\n",
    "\n",
    "# Clean the 'Date_UTC' column to handle invalid entries\n",
    "data_sme['Date_UTC'] = data_sme['Date_UTC'].str.strip()  # Remove leading/trailing whitespace\n",
    "data_sme = data_sme[~data_sme['Date_UTC'].str.contains('<br />|Fatal error', na=False)]  # Remove invalid rows\n",
    "\n",
    "# Convert 'Date_UTC' to datetime\n",
    "data_sme['Date_UTC'] = pd.to_datetime(data_sme['Date_UTC'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaT in 'Date_UTC'\n",
    "data_sme = data_sme.dropna(subset=['Date_UTC'])\n",
    "\n",
    "# Select data only after 2010-01-01\n",
    "data_sme = data_sme[data_sme['Date_UTC'] > '2010-01-01']\n",
    "\n",
    "print(data_sme.head())\n",
    "print(data_sme.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data_sme['SME'].dropna(), bins=np.logspace(np.log10(1), np.log10(2000), 30), edgecolor='black', alpha=0.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
