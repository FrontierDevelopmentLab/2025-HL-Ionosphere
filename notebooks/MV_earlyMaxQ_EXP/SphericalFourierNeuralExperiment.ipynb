{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7bda12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Optional, Dict, Tuple, List\n",
    "\n",
    "# ==========================================\n",
    "# 1. DATASET LOADING & DATALOADER CREATION\n",
    "# ==========================================\n",
    "# -- Import your custom composite dataset class\n",
    "from src.dataset.composite_dataset import CompositeDataset  # Update if your path differs\n",
    "\n",
    "# -- Define file paths (set these to your actual data locations)\n",
    "gim_parquet_file = '/path/to/gim_data.parquet'\n",
    "celestrak_data_file = '/path/to/celestrak_data.parquet'\n",
    "solar_index_data_file = '/path/to/solar_indices.parquet'\n",
    "omniweb_dir = '/path/to/omniweb_dir'\n",
    "date_start = None   # Use datetimes or None for full range\n",
    "date_end = None\n",
    "normalize = True\n",
    "\n",
    "# -- Create the full dataset and split into training/validation sets\n",
    "full_dataset = CompositeDataset(\n",
    "    gim_parquet_file=gim_parquet_file,\n",
    "    celestrak_data_file=celestrak_data_file,\n",
    "    solar_index_data_file=solar_index_data_file,\n",
    "    omniweb_dir=omniweb_dir,\n",
    "    date_start=date_start,\n",
    "    date_end=date_end,\n",
    "    normalize=normalize\n",
    ")\n",
    "n_total = len(full_dataset)\n",
    "n_train = int(0.9 * n_total)\n",
    "n_val = n_total - n_train\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [n_train, n_val])\n",
    "\n",
    "# -- Collate function: Batches nested dicts to batched tensors\n",
    "def multitask_collate(batch: List[Dict]):\n",
    "    out = {}\n",
    "    for key in batch[0]:\n",
    "        if isinstance(batch[0][key], dict):\n",
    "            out[key] = {subkey: torch.stack([item[key][subkey] for item in batch]) for subkey in batch[0][key]}\n",
    "        else:\n",
    "            out[key] = torch.stack([item[key] for item in batch])\n",
    "    return out\n",
    "\n",
    "# -- Dataloaders\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=multitask_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=multitask_collate)\n",
    "\n",
    "# -- Optional: Sanity check the loader shapes\n",
    "for batch in train_loader:\n",
    "    print(\"Batch keys:\", batch.keys())\n",
    "    print(\"GIM input shape:\", batch['gim']['input'].shape)\n",
    "    print(\"GIM target shape:\", batch['gim']['target'].shape)\n",
    "    print(\"Solar index input shape:\", batch['solar_index']['input'].shape)\n",
    "    print(\"OMNI input shape:\", batch['omniweb']['input'].shape)\n",
    "    print(\"Celestrak input shape:\", batch['celestrak']['input'].shape)\n",
    "    break\n",
    "\n",
    "# ==========================================\n",
    "# 2. MODEL DEFINITION: SPHERICAL FOURIER FNO\n",
    "# ==========================================\n",
    "class SphericalFourierLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Spectral convolution over 2D lat/lon grid.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, modes_lat, modes_lon):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(in_channels, out_channels, modes_lat, modes_lon, 2) * 0.01\n",
    "        )\n",
    "\n",
    "    def compl_mul2d(self, x, w):\n",
    "        w = torch.view_as_complex(w)\n",
    "        return torch.einsum(\"bchw,cohw->bohw\", x, w)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x_ft = torch.fft.rfft2(x, norm=\"ortho\")\n",
    "        x_ft = x_ft[:, :, :self.weight.shape[2], :self.weight.shape[3]]\n",
    "        out_ft = self.compl_mul2d(x_ft, self.weight)\n",
    "        out_ft_full = torch.zeros(B, self.weight.shape[1], H, W//2+1, dtype=torch.cfloat, device=x.device)\n",
    "        out_ft_full[:, :, :self.weight.shape[2], :self.weight.shape[3]] = out_ft\n",
    "        return torch.fft.irfft2(out_ft_full, s=(H, W), norm=\"ortho\")\n",
    "\n",
    "class MCDropout(nn.Dropout):\n",
    "    \"\"\"\n",
    "    Dropout that can be forced ON at eval time for MC uncertainty.\n",
    "    \"\"\"\n",
    "    def __init__(self, p=0.1):\n",
    "        super().__init__(p)\n",
    "        self.mc = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.dropout(x, self.p, training=self.training or self.mc)\n",
    "\n",
    "class SFNOBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single block: Fourier layer + local 1x1 conv + MC Dropout + LayerNorm.\n",
    "    \"\"\"\n",
    "    def __init__(self, width, modes_lat, modes_lon, dropout=0.0, mc_dropout=False):\n",
    "        super().__init__()\n",
    "        self.fourier = SphericalFourierLayer(width, width, modes_lat, modes_lon)\n",
    "        self.linear = nn.Conv2d(width, width, 1)\n",
    "        self.dropout = MCDropout(dropout)\n",
    "        self.dropout.mc = mc_dropout\n",
    "        self.norm = nn.LayerNorm([width])\n",
    "\n",
    "    def set_mc_dropout(self, mc=True):\n",
    "        self.dropout.mc = mc\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.fourier(x)\n",
    "        x2 = self.linear(x)\n",
    "        out = x1 + x2\n",
    "        out = self.dropout(out)\n",
    "        # LayerNorm over channel\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "        out = self.norm(out)\n",
    "        out = out.permute(0, 3, 1, 2)\n",
    "        return out\n",
    "\n",
    "class SFNOMultiTask(nn.Module):\n",
    "    \"\"\"\n",
    "    Full Spherical FNO, multi-task, multi-step, probabilistic.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels: int, trunk_width: int = 64, trunk_depth: int = 6,\n",
    "        modes_lat: int = 32, modes_lon: int = 64,\n",
    "        aux_dim: int = 0, tasks: Tuple[str] = (\"vtec\",),\n",
    "        out_shapes: Dict[str, Tuple[int, str]] = {\"vtec\": (1, \"grid\")},\n",
    "        probabilistic: bool = True, dropout: float = 0.1, mc_dropout: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Conv2d(in_channels, trunk_width, 1)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SFNOBlock(trunk_width, modes_lat, modes_lon, dropout=dropout, mc_dropout=mc_dropout)\n",
    "            for _ in range(trunk_depth)\n",
    "        ])\n",
    "        self.aux_proj = nn.Linear(aux_dim, trunk_width) if aux_dim > 0 else None\n",
    "        self.tasks = tasks\n",
    "        self.out_shapes = out_shapes\n",
    "        self.probabilistic = probabilistic\n",
    "\n",
    "        # One head per task\n",
    "        self.heads = nn.ModuleDict()\n",
    "        for task in tasks:\n",
    "            out_dim, out_type = out_shapes[task]\n",
    "            out_channels = out_dim * (2 if probabilistic else 1)\n",
    "            if out_type == \"grid\":\n",
    "                self.heads[task] = nn.Conv2d(trunk_width, out_channels, 1)\n",
    "            elif out_type == \"scalar\":\n",
    "                self.heads[task] = nn.Sequential(\n",
    "                    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(trunk_width, out_channels)\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown head type: {out_type}\")\n",
    "\n",
    "    def set_mc_dropout(self, mc=True):\n",
    "        for b in self.blocks:\n",
    "            b.set_mc_dropout(mc)\n",
    "\n",
    "    def forward(self, x, aux: Optional[torch.Tensor] = None):\n",
    "        x = self.in_proj(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        if self.aux_proj is not None and aux is not None:\n",
    "            aux_emb = self.aux_proj(aux).unsqueeze(-1).unsqueeze(-1)\n",
    "            x = x + aux_emb\n",
    "        outputs = {}\n",
    "        for task in self.tasks:\n",
    "            head = self.heads[task]\n",
    "            out_dim, out_type = self.out_shapes[task]\n",
    "            y = head(x)\n",
    "            if self.probabilistic:\n",
    "                split = out_dim\n",
    "                mean, logvar = y[:, :split], y[:, split:]\n",
    "                outputs[task] = (mean, logvar)\n",
    "            else:\n",
    "                outputs[task] = y\n",
    "        return outputs\n",
    "\n",
    "# ==========================================\n",
    "# 3. TRAINING/VALIDATION ROUTINES\n",
    "# ==========================================\n",
    "def nll_gaussian(pred, logvar, target, reduce=True):\n",
    "    \"\"\"Negative log-likelihood for Gaussian regression.\"\"\"\n",
    "    loss = 0.5 * (logvar + ((pred - target) ** 2) / logvar.exp())\n",
    "    return loss.mean() if reduce else loss\n",
    "\n",
    "def train_epoch(model, loader, optimizer, device=\"cuda\", tasks=(\"vtec\",), reduce=True):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        x = batch['gim']['input'].to(device)      # (B, Cin, H, W)\n",
    "        y = batch['gim']['target'].to(device)     # (B, Cout, H, W)\n",
    "        aux = torch.cat([\n",
    "            batch['solar_index']['input'],\n",
    "            batch['omniweb']['input'],\n",
    "            batch['celestrak']['input']\n",
    "        ], dim=-1).to(device)\n",
    "        targets = {'vtec': y}  # Add more as needed\n",
    "        outputs = model(x, aux)\n",
    "        loss = 0.0\n",
    "        for task in tasks:\n",
    "            if model.probabilistic:\n",
    "                pred, logvar = outputs[task]\n",
    "                tgt = targets[task]\n",
    "                loss += nll_gaussian(pred, logvar, tgt, reduce=reduce)\n",
    "            else:\n",
    "                loss += F.mse_loss(outputs[task], targets[task])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.shape[0]\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, device=\"cuda\", tasks=(\"vtec\",), mc_dropout_samples: int = 1):\n",
    "    model.eval()\n",
    "    orig_mc = getattr(model.blocks[0].dropout, \"mc\", False)\n",
    "    all_mc_outputs = []\n",
    "    for _ in range(mc_dropout_samples):\n",
    "        model.set_mc_dropout(mc_dropout_samples > 1)\n",
    "        batch_outputs = []\n",
    "        for batch in loader:\n",
    "            x = batch['gim']['input'].to(device)\n",
    "            y = batch['gim']['target'].to(device)\n",
    "            aux = torch.cat([\n",
    "                batch['solar_index']['input'],\n",
    "                batch['omniweb']['input'],\n",
    "                batch['celestrak']['input']\n",
    "            ], dim=-1).to(device)\n",
    "            out = model(x, aux)\n",
    "            batch_outputs.append({k: (v[0].cpu(), v[1].cpu()) if isinstance(v, tuple) else v.cpu() for k, v in out.items()})\n",
    "        all_mc_outputs.append(batch_outputs)\n",
    "    model.set_mc_dropout(orig_mc)\n",
    "    return all_mc_outputs\n",
    "\n",
    "def compute_total_uncertainty(mc_outputs, task=\"vtec\"):\n",
    "    \"\"\"Aggregate MC dropout samples for full uncertainty.\"\"\"\n",
    "    preds = torch.stack([b[0][task][0] for b in mc_outputs], dim=0)\n",
    "    logvars = torch.stack([b[0][task][1] for b in mc_outputs], dim=0)\n",
    "    epistemic = preds.std(dim=0)\n",
    "    aleatoric = logvars.exp().mean(dim=0)\n",
    "    total_var = epistemic ** 2 + aleatoric\n",
    "    total_std = total_var.sqrt()\n",
    "    return preds.mean(dim=0), epistemic, aleatoric, total_std\n",
    "\n",
    "# ==========================================\n",
    "# 4. TRAINING LOOP & MODEL INSTANTIATION\n",
    "# ==========================================\n",
    "tasks = (\"vtec\", \"kp\", \"dst\")\n",
    "out_shapes = {\n",
    "    \"vtec\": (3, \"grid\"),  # Predict 3 time steps of VTEC on grid\n",
    "    \"kp\": (2, \"scalar\"),  # 2-step Kp\n",
    "    \"dst\": (2, \"scalar\"), # 2-step Dst\n",
    "}\n",
    "aux_dim = 20  # Sum of auxiliary feature dims (edit if needed)\n",
    "\n",
    "model = SFNOMultiTask(\n",
    "    in_channels=3, trunk_width=64, trunk_depth=8, modes_lat=32, modes_lon=64,\n",
    "    aux_dim=aux_dim, tasks=tasks, out_shapes=out_shapes,\n",
    "    probabilistic=True, dropout=0.2, mc_dropout=True\n",
    ")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "# -- Main Training Loop\n",
    "for epoch in range(30):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device, tasks)\n",
    "    val_outputs = eval_epoch(model, val_loader, device, tasks, mc_dropout_samples=10)\n",
    "    print(f\"Epoch {epoch:02d}: Train Loss={train_loss:.4f} | MC samples={len(val_outputs)}\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. AUTOREGRESSIVE ROLLOUT FOR FORECASTING\n",
    "# ==========================================\n",
    "def autoregressive_rollout(model, init_input, aux_seq, horizon=3, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Roll forward for multiple timesteps (autoregressive inference).\n",
    "    Args:\n",
    "        model: Trained SFNO model\n",
    "        init_input: (B, Cin, H, W) initial input window\n",
    "        aux_seq: (B, horizon, aux_dim) sequence of aux features\n",
    "    Returns:\n",
    "        (B, horizon, Cout, H, W): predictions for each future step\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    cur_input = init_input.to(device)\n",
    "    for t in range(horizon):\n",
    "        aux_t = aux_seq[:, t].to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(cur_input, aux_t)\n",
    "            pred_mean, _ = out['vtec']  # (B, Cout, H, W)\n",
    "            preds.append(pred_mean.unsqueeze(1))\n",
    "            # Slide window: drop oldest, append new\n",
    "            cur_input = torch.cat([cur_input[:, 1:], pred_mean], dim=1)\n",
    "    return torch.cat(preds, dim=1)  # (B, horizon, Cout, H, W)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
