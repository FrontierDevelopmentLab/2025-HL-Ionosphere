{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c9cddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import pprint\n",
    "import os\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import cartopy.crs as ccrs\n",
    "import glob\n",
    "\n",
    "\n",
    "from util import Tee\n",
    "from util import set_random_seed\n",
    "from models_conditioned import VAE1, IonCastConvLSTM\n",
    "from datasets import JPLD # Sequences #, UnionDataset\n",
    "from src import OMNIDataset, CelestrakDataset, SolarIndexDataset, UnionDataset, Sequences, CompositeDataset\n",
    "from events import EventCatalog\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "\n",
    "def plot_global_ionosphere_map(ax, image, cmap='jet', vmin=None, vmax=None, title=None):\n",
    "    \"\"\"\n",
    "    Plots a 180x360 global ionosphere image on a given Cartopy axes.\n",
    "    \n",
    "    Parameters:\n",
    "        ax (matplotlib.axes._subplots.AxesSubplot): Axes with a Cartopy projection.\n",
    "        image (np.ndarray): 2D numpy array with shape (180, 360), representing lat [-90, 90], lon [-180, 180].\n",
    "        cmap (str): Colormap to use for imshow.\n",
    "        vmin (float): Minimum value for colormap normalization.\n",
    "        vmax (float): Maximum value for colormap normalization.\n",
    "    \"\"\"\n",
    "    if image.shape != (180, 360):\n",
    "        raise ValueError(\"Input image must have shape (180, 360), but got shape {}.\".format(image.shape))\n",
    "\n",
    "    im = ax.imshow(\n",
    "        image,\n",
    "        extent=[-180, 180, -90, 90],\n",
    "        origin='lower',\n",
    "        cmap=cmap,\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "        transform=ccrs.PlateCarree()\n",
    "    )\n",
    "    \n",
    "    ax.coastlines()\n",
    "    if title is not None:\n",
    "        ax.set_title(title, fontsize=12)\n",
    "\n",
    "    return im\n",
    "\n",
    "\n",
    "def save_gim_plot(gim, file_name, cmap='jet', vmin=None, vmax=None, title=None):\n",
    "    \"\"\"\n",
    "    Plots a single 180x360 global ionosphere image using GridSpec,\n",
    "    with a colorbar aligned to the full height of the imshow map.\n",
    "    \"\"\"\n",
    "    print(f'Plotting GIM to {file_name}')\n",
    "    \n",
    "    if gim.shape != (180, 360):\n",
    "        raise ValueError(\"Input image must have shape (180, 360) corresponding to lat [-90, 90], lon [-180, 180].\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # GridSpec: one row, two columns\n",
    "    gs = fig.add_gridspec(\n",
    "        1, 2, width_ratios=[20, 1], wspace=0.05,\n",
    "        left=0.05, right=0.98, top=0.9, bottom=0.1\n",
    "    )\n",
    "    \n",
    "    # Main plot\n",
    "    ax = fig.add_subplot(gs[0, 0], projection=ccrs.PlateCarree())\n",
    "    im = plot_global_ionosphere_map(ax, gim, cmap=cmap, vmin=vmin, vmax=vmax, title=title)\n",
    "    \n",
    "    # Colorbar axis â€” NOT a projection axis\n",
    "    cbar_ax = fig.add_subplot(gs[0, 1])\n",
    "    cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "    cbar.set_label(\"TEC (TECU)\")\n",
    "    \n",
    "    plt.savefig(file_name, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Save a sequence of GIM images as a video, exactly the same as save_gim_plot but for a sequence of images\n",
    "def save_gim_video(gim_sequence, file_name, cmap='jet', vmin=None, vmax=None, titles=None, fps=2):\n",
    "    # gim_sequence has shape (num_frames, 180, 360)\n",
    "    print(f'Saving GIM video to {file_name}')\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    gs = fig.add_gridspec(1, 2, width_ratios=[20, 1], wspace=0.05, left=0.05, right=0.92, top=0.9, bottom=0.1)\n",
    "    ax = fig.add_subplot(gs[0, 0], projection=ccrs.PlateCarree())\n",
    "    cbar_ax = fig.add_subplot(gs[0, 1])\n",
    "    \n",
    "    # Initialize with first frame\n",
    "    im = plot_global_ionosphere_map(ax, gim_sequence[0], cmap=cmap, vmin=vmin, vmax=vmax, \n",
    "                                   title=titles[0] if titles else None)\n",
    "    cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "    cbar.set_label(\"TEC (TECU)\")\n",
    "\n",
    "    def update(frame):\n",
    "        # Update the image data instead of clearing\n",
    "        new_im = plot_global_ionosphere_map(ax, gim_sequence[frame], cmap=cmap, vmin=vmin, vmax=vmax, \n",
    "                                           title=titles[frame] if titles else None)\n",
    "        return [new_im]\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, update, frames=len(gim_sequence), blit=False, \n",
    "                                 interval=1000/fps, repeat=False)\n",
    "    ani.save(file_name, dpi=150, writer='ffmpeg', extra_args=['-pix_fmt', 'yuv420p'])\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_gim_video_comparison(gim_sequence_top, gim_sequence_bottom, file_name, cmap='jet', vmin=None, vmax=None, \n",
    "                             titles_top=None, titles_bottom=None, fps=2):\n",
    "    \"\"\"\n",
    "    Save two GIM sequences as a comparison video with 2x1 grid (top and bottom).\n",
    "    \n",
    "    Parameters:\n",
    "        gim_sequence_top: numpy array of shape (num_frames, 180, 360) for the top video\n",
    "        gim_sequence_bottom: numpy array of shape (num_frames, 180, 360) for the bottom video\n",
    "        file_name: output video file name\n",
    "        cmap: colormap to use\n",
    "        vmin, vmax: color scale limits\n",
    "        titles_top: list of titles for top video frames\n",
    "        titles_bottom: list of titles for bottom video frames\n",
    "        fps: frames per second\n",
    "    \"\"\"\n",
    "    # Ensure both sequences have the same length\n",
    "    if len(gim_sequence_top) != len(gim_sequence_bottom):\n",
    "        raise ValueError(f\"Sequences must have same length: {len(gim_sequence_top)} vs {len(gim_sequence_bottom)}\")\n",
    "    \n",
    "    print(f'Saving GIM video to {file_name}')\n",
    "\n",
    "    # Create figure with 2 rows, 2 columns (maps + colorbars)\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    gs = fig.add_gridspec(2, 2, width_ratios=[20, 1], height_ratios=[1, 1], \n",
    "                         wspace=0.05, hspace=0.15, left=0.05, right=0.92, top=0.95, bottom=0.05)\n",
    "    \n",
    "    # Top subplot (original/real data)\n",
    "    ax_top = fig.add_subplot(gs[0, 0], projection=ccrs.PlateCarree())\n",
    "    cbar_ax_top = fig.add_subplot(gs[0, 1])\n",
    "    \n",
    "    # Bottom subplot (forecast/predicted data)\n",
    "    ax_bottom = fig.add_subplot(gs[1, 0], projection=ccrs.PlateCarree())\n",
    "    cbar_ax_bottom = fig.add_subplot(gs[1, 1])\n",
    "    \n",
    "    # Initialize with first frame\n",
    "    im_top = plot_global_ionosphere_map(ax_top, gim_sequence_top[0], cmap=cmap, vmin=vmin, vmax=vmax, \n",
    "                                       title=titles_top[0] if titles_top else None)\n",
    "    cbar_top = fig.colorbar(im_top, cax=cbar_ax_top)\n",
    "    cbar_top.set_label(\"TEC (TECU)\")\n",
    "    \n",
    "    im_bottom = plot_global_ionosphere_map(ax_bottom, gim_sequence_bottom[0], cmap=cmap, vmin=vmin, vmax=vmax, \n",
    "                                          title=titles_bottom[0] if titles_bottom else None)\n",
    "    cbar_bottom = fig.colorbar(im_bottom, cax=cbar_ax_bottom)\n",
    "    cbar_bottom.set_label(\"TEC (TECU)\")\n",
    "\n",
    "    def update(frame):\n",
    "        # Update top plot\n",
    "        new_im_top = plot_global_ionosphere_map(ax_top, gim_sequence_top[frame], cmap=cmap, vmin=vmin, vmax=vmax, \n",
    "                                               title=titles_top[frame] if titles_top else None)\n",
    "        \n",
    "        # Update bottom plot\n",
    "        new_im_bottom = plot_global_ionosphere_map(ax_bottom, gim_sequence_bottom[frame], cmap=cmap, vmin=vmin, vmax=vmax, \n",
    "                                                  title=titles_bottom[frame] if titles_bottom else None)\n",
    "        \n",
    "        return [new_im_top, new_im_bottom]\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, update, frames=len(gim_sequence_top), blit=False, \n",
    "                                 interval=1000/fps, repeat=False)\n",
    "    ani.save(file_name, dpi=150, writer='ffmpeg', extra_args=['-pix_fmt', 'yuv420p'])\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def run_forecast(model, dataset, date_start, date_end, date_forecast_start, title, file_name, args):\n",
    "    if not isinstance(model, (IonCastConvLSTM)):\n",
    "        raise ValueError('Model must be an instance of IonCastConvLSTM')\n",
    "    if date_start > date_end:\n",
    "        raise ValueError('date_start must be before date_end')\n",
    "    if date_forecast_start - datetime.timedelta(minutes=model.context_window * args.delta_minutes) < date_start:\n",
    "        raise ValueError('date_forecast_start must be at least context_window * delta_minutes after date_start')\n",
    "    if date_forecast_start >= date_end:\n",
    "        raise ValueError('date_forecast_start must be before date_end')\n",
    "    # date_forecast_start must be an integer multiple of args.delta_minutes from date_start\n",
    "    if (date_forecast_start - date_start).total_seconds() % (args.delta_minutes * 60) != 0:\n",
    "        raise ValueError('date_forecast_start must be an integer multiple of args.delta_minutes from date_start')\n",
    "\n",
    "    print('Evaluation from {} to {}'.format(date_start, date_end))\n",
    "    print('Forecast start date: {}'.format(date_forecast_start))\n",
    "    sequence_start = date_start\n",
    "    sequence_end = date_end\n",
    "    sequence_length = int((sequence_end - sequence_start).total_seconds() / 60 / args.delta_minutes)\n",
    "    print('Sequence length: {}'.format(sequence_length))\n",
    "    sequence = [sequence_start + datetime.timedelta(minutes=args.delta_minutes * i) for i in range(sequence_length)]\n",
    "    # find the index of the date_forecast_start in the list sequence\n",
    "    if date_forecast_start not in sequence:\n",
    "        raise ValueError('date_forecast_start must be in the sequence')\n",
    "    sequence_forecast_start_index = sequence.index(date_forecast_start)\n",
    "    sequence_prediction_window = sequence_length - (sequence_forecast_start_index) # TODO: should this be sequence_length - (sequence_forecast_start_index + 1)\n",
    "\n",
    "    sequence_data = dataset.get_sequence_data(sequence)\n",
    "    jpld_original = sequence_data[0]  # Original data\n",
    "    device = next(model.parameters()).device\n",
    "    jpld_original = jpld_original.to(device)\n",
    "    jpld_forecast_context = jpld_original[:sequence_forecast_start_index]  # Context data for forecast\n",
    "    jpld_forecast = model.predict(jpld_forecast_context.unsqueeze(0), prediction_window=sequence_prediction_window).squeeze(0)\n",
    "\n",
    "    print(jpld_original.shape)\n",
    "    print(jpld_forecast_context.shape)\n",
    "    print(jpld_forecast.shape)\n",
    "\n",
    "    jpld_forecast_with_context = torch.cat((jpld_forecast_context, jpld_forecast), dim=0)\n",
    "\n",
    "    jpld_original_unnormalized = JPLD.unnormalize(jpld_original)\n",
    "    jpld_forecast_with_context_unnormalized = JPLD.unnormalize(jpld_forecast_with_context)\n",
    "    jpld_forecast_with_context_unnormalized = jpld_forecast_with_context_unnormalized.clamp(0, 100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # comparison_video_file = os.path.join(args.target_dir, file_name)\n",
    "    # save_gim_video_comparison(\n",
    "    #     original.cpu().numpy().reshape(args.prediction_window, 180, 360),\n",
    "    #     forecast.cpu().numpy().reshape(args.prediction_window, 180, 360),\n",
    "    #     comparison_video_file,\n",
    "    #     vmin=0, vmax=100,\n",
    "    #     titles_top=[f'{title} Original: {d}' for d in dates],\n",
    "    #     titles_bottom=[f'{title} Forecast: {d}' for d in dates]\n",
    "    # )\n",
    "\n",
    "    forecast_mins_ahead = ['{} mins'.format((j + 1) * 15) for j in range(sequence_prediction_window)]\n",
    "    titles_original = [f'{title} JPLD GIM TEC Original: {d}' for d in sequence]\n",
    "    titles_forecast = []\n",
    "    for i in range(sequence_length):\n",
    "        if i < sequence_forecast_start_index:\n",
    "            titles_forecast.append(f'{title} JPLD GIM TEC Original (Context): {sequence[i]}')\n",
    "        else:\n",
    "            titles_forecast.append(f'{title} JPLD GIM TEC Forecast: {sequence[i]} ({forecast_mins_ahead[i - sequence_forecast_start_index]})')\n",
    "\n",
    "    # print(jpld_original_unnormalized.shape)\n",
    "    # print(jpld_forecast_with_context_unnormalized.shape)\n",
    "    # print(len(titles_original))\n",
    "    # print(len(titles_forecast))\n",
    "    save_gim_video_comparison(\n",
    "        gim_sequence_top=jpld_original_unnormalized.cpu().numpy().reshape(-1, 180, 360),\n",
    "        gim_sequence_bottom=jpld_forecast_with_context_unnormalized.cpu().numpy().reshape(-1, 180, 360),\n",
    "        file_name=file_name,\n",
    "        vmin=0, vmax=100,\n",
    "        titles_top=titles_original,\n",
    "        titles_bottom=titles_forecast\n",
    "    )\n",
    "\n",
    "def save_model(model, optimizer, epoch, iteration, train_losses, valid_losses, file_name):\n",
    "    print('Saving model to {}'.format(file_name))\n",
    "    if isinstance(model, VAE1):\n",
    "        checkpoint = {\n",
    "            'model': 'VAE1',\n",
    "            'epoch': epoch,\n",
    "            'iteration': iteration,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'valid_losses': valid_losses,\n",
    "            'model_z_dim': model.z_dim,\n",
    "            'model_c_dim': model.c_dim,\n",
    "        }\n",
    "    elif isinstance(model, IonCastConvLSTM):\n",
    "        checkpoint = {\n",
    "            'model': 'IonCastConvLSTM',\n",
    "            'epoch': epoch,\n",
    "            'iteration': iteration,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'valid_losses': valid_losses,\n",
    "            'model_context_window': model.context_window,\n",
    "            'model_prediction_window': model.prediction_window,\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError('Unknown model type: {}'.format(model))\n",
    "    torch.save(checkpoint, file_name)\n",
    "\n",
    "\n",
    "def load_model(file_name, device):\n",
    "    checkpoint = torch.load(file_name, weights_only=False)\n",
    "    if checkpoint['model'] == 'VAE1':\n",
    "        model_z_dim = checkpoint['model_z_dim']\n",
    "        model_c_dim = checkpoint['model_c_dim']\n",
    "        model = VAE1(z_dim=model_z_dim, c_dim=model_c_dim) # c_dim is the dimenion of the \n",
    "    elif checkpoint['model'] == 'IonCastConvLSTM':\n",
    "        model_context_window = checkpoint['model_context_window']\n",
    "        model_prediction_window = checkpoint['model_prediction_window']\n",
    "        model = IonCastConvLSTM(context_window=model_context_window, prediction_window=model_prediction_window)\n",
    "    else:\n",
    "        raise ValueError('Unknown model type: {}'.format(checkpoint['model']))\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    iteration = checkpoint['iteration']\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    valid_losses = checkpoint['valid_losses']\n",
    "    return model, optimizer, epoch, iteration, train_losses, valid_losses\n",
    "\n",
    "\n",
    "\n",
    "# description = 'NASA Heliolab 2025 - Ionosphere-Thermosphere Twin, ML experiments'\n",
    "# parser = argparse.ArgumentParser(description=description)\n",
    "# parser.add_argument('--data_dir', type=str, required=True, help='Root directory for the datasets')\n",
    "# parser.add_argument('--jpld_dir', type=str, default='jpld/webdataset', help='JPLD GIM dataset directory')\n",
    "# parser.add_argument('--omni_dir', type=str, default='omniweb/cleaned/', help='OMNIWeb dataset directory')\n",
    "# parser.add_argument('--celestrak_file', type=str, default='celestrak/kp_ap_processed_timeseries.csv', help='Celestrak dataset csv file')\n",
    "# parser.add_argument('--solar_index_file', type=str, default='solar_env_tech_indices/Indices_F10_processed.csv', help='Solar indices dataset csv file')\n",
    "# parser.add_argument('--aux_datasets', nargs='+', choices=[\"omni\", \"celestrak\", \"solar_inds\"], default=[\"omni\", \"celestrak\", \"solar_inds\"], help=\"additional datasets to include on top of TEC maps\")\n",
    "\n",
    "# parser.add_argument('--target_dir', type=str, help='Directory to save the statistics', required=True)\n",
    "# # parser.add_argument('--date_start', type=str, default='2010-05-13T00:00:00', help='Start date')\n",
    "# # parser.add_argument('--date_end', type=str, default='2024-08-01T00:00:00', help='End date')\n",
    "# parser.add_argument('--date_start', type=str, default='2023-07-01T00:00:00', help='Start date')\n",
    "# parser.add_argument('--date_end', type=str, default='2023-07-03T00:00:00', help='End date')\n",
    "# parser.add_argument('--delta_minutes', type=int, default=15, help='Time step in minutes')\n",
    "# parser.add_argument('--seed', type=int, default=0, help='Random seed for reproducibility')\n",
    "# parser.add_argument('--epochs', type=int, default=2, help='Number of epochs for training')\n",
    "# parser.add_argument('--batch_size', type=int, default=32, help='Batch size for training')\n",
    "# parser.add_argument('--learning_rate', type=float, default=3e-4, help='Learning rate')\n",
    "# parser.add_argument('--weight_decay', type=float, default=0, help='Weight decay')    \n",
    "# parser.add_argument('--mode', type=str, choices=['train', 'test'], required=True, help='Mode of operation: train or test')\n",
    "# parser.add_argument('--model_type', type=str, choices=['VAE1', 'IonCastConvLSTM'], default='VAE1', help='Type of model to use')\n",
    "# parser.add_argument('--num_workers', type=int, default=4, help='Number of workers for data loading')\n",
    "# parser.add_argument('--device', type=str, default='cpu', help='Device')\n",
    "# parser.add_argument('--num_evals', type=int, default=4, help='Number of samples for evaluation')\n",
    "# parser.add_argument('--context_window', type=int, default=4, help='Context window size for the model')\n",
    "# parser.add_argument('--prediction_window', type=int, default=4, help='Evaluation window size for the model')\n",
    "# parser.add_argument('--test_event_id', nargs='+', default=['G2H9-202311050900', 'G2H9-202405101500', 'G2H9-202406280900'], help='Test event IDs to use for evaluation')\n",
    "# parser.add_argument('--test_event_seen_id', nargs='+', default=['G2H9-202111032100', 'G2H9-202303230900', 'G2H9-202304231200'], help='Test event IDs that the model has seen during training')\n",
    "# # parser.add_argument('--test_event_id', nargs='+', default=['G2H9-202311050900'], help='Test event IDs to use for evaluation')\n",
    "# # parser.add_argument('--test_event_seen_id', nargs='+', default=['G2H9-202111032100'], help='Test event IDs that the model has seen during training')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example\n",
    "# python run.py --data_dir /disk2-ssd-8tb/data/2025-hl-ionosphere --mode train --target_dir ./train-1 --num_workers 4 --batch_size 4 --model_type IonCastConvLSTM --epochs 2 --learning_rate 1e-3 --weight_decay 0.0 --context_window 4 --prediction_window 4 --num_evals 4 --date_start 2023-07-01T00:00:00 --date_end 2023-08-01T00:00:00\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18e83e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "description = 'NASA Heliolab 2025 - Ionosphere-Thermosphere Twin, ML experiments'\n",
    "\n",
    "args = Namespace(\n",
    "\n",
    "    data_dir='/mnt/disks/disk-main-data-1/data/',\n",
    "    jpld_dir='jpld/webdataset',\n",
    "    omni_dir='omniweb/cleaned/',\n",
    "    celestrak_file='celestrak/kp_ap_processed_timeseries.csv',\n",
    "    solar_index_file='solar_env_tech_indices/Indices_F10_processed.csv',\n",
    "    aux_datasets=[\"omni\", \"celestrak\", \"solar_inds\"],\n",
    "\n",
    "    target_dir='./train-conditioned-2',\n",
    "    date_start='2020-10-01T00:00:00',\n",
    "    date_end='2020-10-05T00:00:00',\n",
    "    delta_minutes=15,\n",
    "\n",
    "    seed=0,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=0.0,\n",
    "    mode='train',\n",
    "    model_type='VAE1',\n",
    "    num_workers=4,\n",
    "    device='cuda',\n",
    "    num_evals=4,\n",
    "    context_window=4,\n",
    "    prediction_window=4,\n",
    "\n",
    "    test_event_id=['G2H9-202311050900', 'G2H9-202405101500', 'G2H9-202406280900'],\n",
    "    test_event_seen_id=['G2H9-202111032100', 'G2H9-202303230900', 'G2H9-202304231200']\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0b7b3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting seed to 0\n",
      "NASA Heliolab 2025 - Ionosphere-Thermosphere Twin, ML experiments\n",
      "Log file: ./train-conditioned-1/log.txt\n",
      "Arguments:\n",
      "--f=/run/user/1005/jupyter/runtime/kernel-v3dea0fe861afb92946af9e1eed6cdfba65c16b94c.json\n",
      "Config:\n",
      "{'aux_datasets': ['omni',\n",
      "                  'celestrak',\n",
      "                  'solar_inds'],\n",
      " 'batch_size': 16,\n",
      " 'celestrak_file': 'celestrak/kp_ap_processed_timeseries.csv',\n",
      " 'context_window': 4,\n",
      " 'data_dir': '/mnt/disks/disk-main-data-1/data/',\n",
      " 'date_end': '2020-10-05T00:00:00',\n",
      " 'date_start': '2020-10-01T00:00:00',\n",
      " 'delta_minutes': 15,\n",
      " 'device': 'cuda',\n",
      " 'epochs': 100,\n",
      " 'jpld_dir': 'jpld/webdataset',\n",
      " 'learning_rate': 0.001,\n",
      " 'mode': 'train',\n",
      " 'model_type': 'VAE1',\n",
      " 'num_evals': 4,\n",
      " 'num_workers': 4,\n",
      " 'omni_dir': 'omniweb/cleaned/',\n",
      " 'prediction_window': 4,\n",
      " 'seed': 0,\n",
      " 'solar_index_file': 'solar_env_tech_indices/Indices_F10_processed.csv',\n",
      " 'target_dir': './train-conditioned-1',\n",
      " 'test_event_id': ['G2H9-202311050900',\n",
      "                   'G2H9-202405101500',\n",
      "                   'G2H9-202406280900'],\n",
      " 'test_event_seen_id': ['G2H9-202111032100',\n",
      "                        'G2H9-202303230900',\n",
      "                        'G2H9-202304231200'],\n",
      " 'weight_decay': 0.0}\n",
      "Start time: 2025-07-29 15:45:06.698756\n",
      "Training mode selected.\n",
      "Processing excluded dates\n",
      "Excluding event ID: G2H9-202311050900\n",
      "\n",
      "JPLD\n",
      "Directory  : /mnt/disks/disk-main-data-1/data/jpld/webdataset\n",
      "Loading tar files index from cache: /mnt/disks/disk-main-data-1/data/jpld/webdataset/tar_files_index\n",
      "Start date : 2023-11-05 03:00:00\n",
      "End date   : 2023-11-06 09:00:00\n",
      "Delta      : 15 minutes\n",
      "Loading dates from cache: /mnt/disks/disk-main-data-1/data/jpld/webdataset/dates_index_2023-11-05T03:00:00_2023-11-06T09:00:00\n",
      "TEC maps total    : 120\n",
      "TEC maps available: 120\n",
      "TEC maps dropped  : 0\n",
      "\n",
      "OMNIWeb dataset\n",
      "File                 : /mnt/disks/disk-main-data-1/data/omniweb/cleaned/omni_5min_full_cleaned.csv\n",
      "Rows                 : 2,103,840\n",
      "Delta minutes        : 15\n",
      "Normalize            : True\n",
      "Rewind minutes       : 50\n",
      "column:['B_mag', 'Bx_GSE', 'By_GSM', 'Bz_GSM', 'RMS_B_scalar', 'RMS_B_vector', 'V_flow', 'Vx', 'Vy', 'Vz', 'Density', 'Temp', 'SYM_D', 'SYM_H', 'ASY_D', 'ASY_H']\n",
      "2103840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hert7450/2025-HL-Ionosphere/src/dataset/base_datasets.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data['Datetime'] = pd.to_datetime(self.data['Datetime']) # this line wasnt present previously but is necessary if the col contains strings instead of pandas timestamps for the date.to_pydatetime() to run as expected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start date           : 2023-11-05 03:00:00\n",
      "End date             : 2023-11-06 09:00:00\n",
      "Rows after processing: 321\n",
      "\n",
      "Celestrak dataset\n",
      "File                 : /mnt/disks/disk-main-data-1/data/celestrak/kp_ap_processed_timeseries.csv\n",
      "Rows                 : 198,320\n",
      "Delta minutes        : 15\n",
      "Normalize            : True\n",
      "Rewind minutes       : 180\n",
      "column:['Kp', 'Ap']\n",
      "2379829\n",
      "Start date           : 2023-11-05 03:00:00\n",
      "End date             : 2023-11-06 09:00:00\n",
      "Rows after processing: 121\n",
      "\n",
      "Solar Index dataset\n",
      "File                 : /mnt/disks/disk-main-data-1/data/solar_env_tech_indices/Indices_F10_processed.csv\n",
      "Rows                 : 10,483\n",
      "Delta minutes        : 15\n",
      "Normalize            : True\n",
      "Rewind minutes       : 1,440\n",
      "column:['F10', 'S10', 'M10', 'Y10']\n",
      "1006273\n",
      "Start date           : 2023-11-05 03:00:00\n",
      "End date             : 2023-11-06 09:00:00\n",
      "Rows after processing: 121\n",
      "Excluding event ID: G2H9-202405101500\n",
      "\n",
      "JPLD\n",
      "Directory  : /mnt/disks/disk-main-data-1/data/jpld/webdataset\n",
      "Loading tar files index from cache: /mnt/disks/disk-main-data-1/data/jpld/webdataset/tar_files_index\n",
      "Start date : 2024-05-09 21:00:00\n",
      "End date   : 2024-05-13 15:00:00\n",
      "Delta      : 15 minutes\n",
      "Loading dates from cache: /mnt/disks/disk-main-data-1/data/jpld/webdataset/dates_index_2024-05-09T21:00:00_2024-05-13T15:00:00\n",
      "TEC maps total    : 360\n",
      "TEC maps available: 360\n",
      "TEC maps dropped  : 0\n",
      "\n",
      "OMNIWeb dataset\n",
      "File                 : /mnt/disks/disk-main-data-1/data/omniweb/cleaned/omni_5min_full_cleaned.csv\n",
      "Rows                 : 2,103,840\n",
      "Delta minutes        : 15\n",
      "Normalize            : True\n",
      "Rewind minutes       : 50\n",
      "column:['B_mag', 'Bx_GSE', 'By_GSM', 'Bz_GSM', 'RMS_B_scalar', 'RMS_B_vector', 'V_flow', 'Vx', 'Vy', 'Vz', 'Density', 'Temp', 'SYM_D', 'SYM_H', 'ASY_D', 'ASY_H']\n",
      "2103840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hert7450/2025-HL-Ionosphere/src/dataset/base_datasets.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data['Datetime'] = pd.to_datetime(self.data['Datetime']) # this line wasnt present previously but is necessary if the col contains strings instead of pandas timestamps for the date.to_pydatetime() to run as expected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start date           : 2024-05-09 21:00:00\n",
      "End date             : 2024-05-13 15:00:00\n",
      "Rows after processing: 885\n",
      "\n",
      "Celestrak dataset\n",
      "File                 : /mnt/disks/disk-main-data-1/data/celestrak/kp_ap_processed_timeseries.csv\n",
      "Rows                 : 198,320\n",
      "Delta minutes        : 15\n",
      "Normalize            : True\n",
      "Rewind minutes       : 180\n",
      "column:['Kp', 'Ap']\n",
      "2379829\n",
      "Start date           : 2024-05-09 21:00:00\n",
      "End date             : 2024-05-13 15:00:00\n",
      "Rows after processing: 361\n",
      "\n",
      "Solar Index dataset\n",
      "File                 : /mnt/disks/disk-main-data-1/data/solar_env_tech_indices/Indices_F10_processed.csv\n",
      "Rows                 : 10,483\n",
      "Delta minutes        : 15\n",
      "Normalize            : True\n",
      "Rewind minutes       : 1,440\n",
      "column:['F10', 'S10', 'M10', 'Y10']\n",
      "1006273\n",
      "Start date           : 2024-05-09 21:00:00\n",
      "End date             : 2024-05-13 15:00:00\n",
      "Rows after processing: 361\n",
      "Excluding event ID: G2H9-202406280900\n",
      "\n",
      "JPLD\n",
      "Directory  : /mnt/disks/disk-main-data-1/data/jpld/webdataset\n",
      "Loading tar files index from cache: /mnt/disks/disk-main-data-1/data/jpld/webdataset/tar_files_index\n",
      "Start date : 2024-06-28 04:30:00\n",
      "End date   : 2024-06-29 03:00:00\n",
      "Delta      : 15 minutes\n",
      "Loading dates from cache: /mnt/disks/disk-main-data-1/data/jpld/webdataset/dates_index_2024-06-28T04:30:00_2024-06-29T03:00:00\n",
      "TEC maps total    : 90\n",
      "TEC maps available: 90\n",
      "TEC maps dropped  : 0\n",
      "\n",
      "OMNIWeb dataset\n",
      "File                 : /mnt/disks/disk-main-data-1/data/omniweb/cleaned/omni_5min_full_cleaned.csv\n",
      "Rows                 : 2,103,840\n",
      "Delta minutes        : 15\n",
      "Normalize            : True\n",
      "Rewind minutes       : 50\n",
      "column:['B_mag', 'Bx_GSE', 'By_GSM', 'Bz_GSM', 'RMS_B_scalar', 'RMS_B_vector', 'V_flow', 'Vx', 'Vy', 'Vz', 'Density', 'Temp', 'SYM_D', 'SYM_H', 'ASY_D', 'ASY_H']\n",
      "2103840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hert7450/2025-HL-Ionosphere/src/dataset/base_datasets.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data['Datetime'] = pd.to_datetime(self.data['Datetime']) # this line wasnt present previously but is necessary if the col contains strings instead of pandas timestamps for the date.to_pydatetime() to run as expected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start date           : 2024-06-28 04:30:00\n",
      "End date             : 2024-06-29 02:25:00\n",
      "Rows after processing: 122\n",
      "\n",
      "Celestrak dataset\n",
      "File                 : /mnt/disks/disk-main-data-1/data/celestrak/kp_ap_processed_timeseries.csv\n",
      "Rows                 : 198,320\n",
      "Delta minutes        : 15\n",
      "Normalize            : True\n",
      "Rewind minutes       : 180\n",
      "column:['Kp', 'Ap']\n",
      "2379829\n",
      "Start date           : 2024-06-28 04:30:00\n",
      "End date             : 2024-06-29 03:00:00\n",
      "Rows after processing: 91\n",
      "\n",
      "Solar Index dataset\n",
      "File                 : /mnt/disks/disk-main-data-1/data/solar_env_tech_indices/Indices_F10_processed.csv\n",
      "Rows                 : 10,483\n",
      "Delta minutes        : 15\n",
      "Normalize            : True\n",
      "Rewind minutes       : 1,440\n",
      "column:['F10', 'S10', 'M10', 'Y10']\n",
      "1006273\n",
      "Start date           : 2024-06-28 04:30:00\n",
      "End date             : 2024-06-29 03:00:00\n",
      "Rows after processing: 91\n",
      "\n",
      "Concatenated datasets\n",
      "Dataset : JPLD (2023-11-05 03:00:00 - 2023-11-06 09:00:00)\n",
      "Dataset : JPLD (2024-05-09 21:00:00 - 2024-05-13 15:00:00)\n",
      "Dataset : JPLD (2024-06-28 04:30:00 - 2024-06-29 03:00:00)\n",
      "expected n_samples:  120\n",
      "expected n_samples:  360\n",
      "expected n_samples:  90\n",
      "570\n",
      "[(datetime.datetime(2023, 11, 5, 3, 0), datetime.datetime(2023, 11, 6, 9, 0)), (datetime.datetime(2024, 5, 9, 21, 0), datetime.datetime(2024, 5, 13, 15, 0)), (datetime.datetime(2024, 6, 28, 4, 30), datetime.datetime(2024, 6, 29, 3, 0))]\n",
      "570\n",
      "[('omni', 3, [OMNIWeb dataset (2023-11-05 03:00:00 - 2023-11-06 09:00:00), OMNIWeb dataset (2024-05-09 21:00:00 - 2024-05-13 15:00:00), OMNIWeb dataset (2024-06-28 04:30:00 - 2024-06-29 02:25:00)]), ('celestrak', 3, [Celestrak dataset (2023-11-05 03:00:00 - 2023-11-06 09:00:00), Celestrak dataset (2024-05-09 21:00:00 - 2024-05-13 15:00:00), Celestrak dataset (2024-06-28 04:30:00 - 2024-06-29 03:00:00)]), ('solar_inds', 3, [Solar Index dataset (2023-11-05 03:00:00 - 2023-11-06 09:00:00), Solar Index dataset (2024-05-09 21:00:00 - 2024-05-13 15:00:00), Solar Index dataset (2024-06-28 04:30:00 - 2024-06-29 03:00:00)])]\n",
      "\n",
      "Concatenated datasets\n",
      "Dataset : OMNIWeb dataset (2023-11-05 03:00:00 - 2023-11-06 09:00:00)\n",
      "Dataset : OMNIWeb dataset (2024-05-09 21:00:00 - 2024-05-13 15:00:00)\n",
      "Dataset : OMNIWeb dataset (2024-06-28 04:30:00 - 2024-06-29 02:25:00)\n",
      "\n",
      "Start and end dates:  2023-11-05 03:00:00 2024-06-29 02:25:00\n",
      "\n",
      "Concatenated datasets\n",
      "Dataset : Celestrak dataset (2023-11-05 03:00:00 - 2023-11-06 09:00:00)\n",
      "Dataset : Celestrak dataset (2024-05-09 21:00:00 - 2024-05-13 15:00:00)\n",
      "Dataset : Celestrak dataset (2024-06-28 04:30:00 - 2024-06-29 03:00:00)\n",
      "\n",
      "Start and end dates:  2023-11-05 03:00:00 2024-06-29 03:00:00\n",
      "\n",
      "Concatenated datasets\n",
      "Dataset : Solar Index dataset (2023-11-05 03:00:00 - 2023-11-06 09:00:00)\n",
      "Dataset : Solar Index dataset (2024-05-09 21:00:00 - 2024-05-13 15:00:00)\n",
      "Dataset : Solar Index dataset (2024-06-28 04:30:00 - 2024-06-29 03:00:00)\n",
      "\n",
      "Start and end dates:  2023-11-05 03:00:00 2024-06-29 03:00:00\n",
      "\n",
      "JPLD\n",
      "Directory  : /mnt/disks/disk-main-data-1/data/jpld/webdataset\n",
      "Loading tar files index from cache: /mnt/disks/disk-main-data-1/data/jpld/webdataset/tar_files_index\n",
      "Start date : 2020-10-01 00:00:00\n",
      "End date   : 2020-10-05 00:00:00\n",
      "Delta      : 15 minutes\n",
      "Date exclusions:\n",
      "  2023-11-05 03:00:00 - 2023-11-06 09:00:00\n",
      "  2024-05-09 21:00:00 - 2024-05-13 15:00:00\n",
      "  2024-06-28 04:30:00 - 2024-06-29 03:00:00\n",
      "Loading dates from cache: /mnt/disks/disk-main-data-1/data/jpld/webdataset/dates_index_2020-10-01T00:00:00_2020-10-05T00:00:00_exclusions__2023-11-05T03:00:00_2023-11-06T09:00:00__2024-05-09T21:00:00_2024-05-13T15:00:00__2024-06-28T04:30:00_2024-06-29T03:00:00\n",
      "TEC maps total    : 384\n",
      "TEC maps available: 384\n",
      "TEC maps dropped  : 0\n",
      "\n",
      "OMNIWeb dataset\n",
      "File                 : /mnt/disks/disk-main-data-1/data/omniweb/cleaned/omni_5min_full_cleaned.csv\n",
      "Rows                 : 2,103,840\n",
      "Delta minutes        : 15\n",
      "Normalize            : True\n",
      "Rewind minutes       : 50\n",
      "column:['B_mag', 'Bx_GSE', 'By_GSM', 'Bz_GSM', 'RMS_B_scalar', 'RMS_B_vector', 'V_flow', 'Vx', 'Vy', 'Vz', 'Density', 'Temp', 'SYM_D', 'SYM_H', 'ASY_D', 'ASY_H']\n",
      "2103840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hert7450/2025-HL-Ionosphere/src/dataset/base_datasets.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data['Datetime'] = pd.to_datetime(self.data['Datetime']) # this line wasnt present previously but is necessary if the col contains strings instead of pandas timestamps for the date.to_pydatetime() to run as expected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date exclusions:\n",
      "  2023-11-05 03:00:00 - 2023-11-06 09:00:00\n",
      "  2024-05-09 21:00:00 - 2024-05-13 15:00:00\n",
      "  2024-06-28 04:30:00 - 2024-06-29 03:00:00\n",
      "Start date           : 2020-10-01 00:00:00\n",
      "End date             : 2020-10-05 00:00:00\n",
      "Rows after processing: 1,050\n",
      "\n",
      "Celestrak dataset\n",
      "File                 : /mnt/disks/disk-main-data-1/data/celestrak/kp_ap_processed_timeseries.csv\n",
      "Rows                 : 198,320\n",
      "Delta minutes        : 15\n",
      "Normalize            : True\n",
      "Rewind minutes       : 180\n",
      "column:['Kp', 'Ap']\n",
      "2379829\n",
      "Date exclusions:\n",
      "  2023-11-05 03:00:00 - 2023-11-06 09:00:00\n",
      "  2024-05-09 21:00:00 - 2024-05-13 15:00:00\n",
      "  2024-06-28 04:30:00 - 2024-06-29 03:00:00\n",
      "Start date           : 2020-10-01 00:00:00\n",
      "End date             : 2020-10-05 00:00:00\n",
      "Rows after processing: 385\n",
      "\n",
      "Solar Index dataset\n",
      "File                 : /mnt/disks/disk-main-data-1/data/solar_env_tech_indices/Indices_F10_processed.csv\n",
      "Rows                 : 10,483\n",
      "Delta minutes        : 15\n",
      "Normalize            : True\n",
      "Rewind minutes       : 1,440\n",
      "column:['F10', 'S10', 'M10', 'Y10']\n",
      "1006273\n",
      "Date exclusions:\n",
      "  2023-11-05 03:00:00 - 2023-11-06 09:00:00\n",
      "  2024-05-09 21:00:00 - 2024-05-13 15:00:00\n",
      "  2024-06-28 04:30:00 - 2024-06-29 03:00:00\n",
      "Start date           : 2020-10-01 00:00:00\n",
      "End date             : 2020-10-05 00:00:00\n",
      "Rows after processing: 385\n",
      "\n",
      "Sequences\n",
      "Start date              : 2020-10-01 00:00:00\n",
      "End date                : 2020-10-05 00:00:00\n",
      "Delta                   : 15 minutes\n",
      "Sequence length         : 1\n",
      "Sequence duration       : 15 minutes\n",
      "Number of sequences     : 347\n",
      "First sequence          : ['2020-10-01T00:00:00']\n",
      "Last sequence           : ['2020-10-04T23:45:00']\n",
      "\n",
      "Sequences\n",
      "Start date              : 2023-11-05 03:00:00\n",
      "End date                : 2024-06-29 02:25:00\n",
      "Delta                   : 15 minutes\n",
      "Sequence length         : 1\n",
      "Sequence duration       : 15 minutes\n",
      "Number of sequences     : 435\n",
      "First sequence          : ['2023-11-05T03:00:00']\n",
      "Last sequence           : ['2024-06-28T16:45:00']\n",
      "347 435\n",
      "[OMNIWeb dataset (2020-10-01 00:00:00 - 2020-10-05 00:00:00), Celestrak dataset (2020-10-01 00:00:00 - 2020-10-05 00:00:00), Solar Index dataset (2020-10-01 00:00:00 - 2020-10-05 00:00:00)]\n",
      "conditioning dimension: 22\n",
      "\n",
      "Train size: 347\n",
      "Valid size: 435\n",
      "28 435\n",
      "End time: 2025-07-29 15:47:52.547198\n",
      "Total duration: 0:02:45.848442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hert7450/2025-HL-Ionosphere/src/dataset/base_datasets.py:116: UserWarning: Should not index dataset by int if aligning multiple datasets, this is error prone if datasets have holes.\n",
      "  warn(\"Should not index dataset by int if aligning multiple datasets, this is error prone if datasets have holes.\")\n",
      "/home/hert7450/2025-HL-Ionosphere/src/dataset/base_datasets.py:116: UserWarning: Should not index dataset by int if aligning multiple datasets, this is error prone if datasets have holes.\n",
      "  warn(\"Should not index dataset by int if aligning multiple datasets, this is error prone if datasets have holes.\")\n",
      "/home/hert7450/2025-HL-Ionosphere/src/dataset/base_datasets.py:116: UserWarning: Should not index dataset by int if aligning multiple datasets, this is error prone if datasets have holes.\n",
      "  warn(\"Should not index dataset by int if aligning multiple datasets, this is error prone if datasets have holes.\")\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(args.target_dir, exist_ok=True)\n",
    "log_file = os.path.join(args.target_dir, 'log.txt')\n",
    "\n",
    "set_random_seed(args.seed)\n",
    "device = torch.device(args.device)\n",
    "\n",
    "with Tee(log_file):\n",
    "    print(description)\n",
    "    print('Log file:', log_file)\n",
    "    print('Arguments:\\n{}'.format(' '.join(sys.argv[1:])))\n",
    "    print('Config:')\n",
    "    pprint.pprint(vars(args), depth=2, width=50)\n",
    "\n",
    "    start_time = datetime.datetime.now()\n",
    "    print('Start time: {}'.format(start_time))\n",
    "\n",
    "    if args.mode == 'train':\n",
    "        print('Training mode selected.')\n",
    "\n",
    "        if args.batch_size < args.num_evals:\n",
    "            print(f'Warning: Batch size {args.batch_size} is less than num_evals {args.num_evals}. Using the batch size for num_evals.')\n",
    "            args.num_evals = args.batch_size\n",
    "\n",
    "        date_start = datetime.datetime.fromisoformat(args.date_start)\n",
    "        date_end = datetime.datetime.fromisoformat(args.date_end)\n",
    "\n",
    "        training_sequence_length = args.context_window + args.prediction_window\n",
    "\n",
    "        dataset_jpld_dir = os.path.join(args.data_dir, args.jpld_dir)\n",
    "\n",
    "        print('Processing excluded dates')\n",
    "        gim_webdataset = os.path.join(args.data_dir, args.jpld_dir)\n",
    "        omni_dir = os.path.join(args.data_dir, args.omni_dir)\n",
    "        celestrak_file = os.path.join(args.data_dir, args.celestrak_file)\n",
    "        solar_index_file = os.path.join(args.data_dir, args.solar_index_file)\n",
    "        # 'jpld': lambda date_exclusion: JPLD(gim_webdataset, date_start=date_start, date_end=date_end, normalize=True, date_exclusions=date_exclusion)\n",
    "        dataset_constructors = {\n",
    "            'omni': lambda date_start_, date_end_, date_exclusions_: OMNIDataset(file_dir=omni_dir, delta_minutes=15, date_start=date_start_, date_end=date_end_, normalize=True, date_exclusions=date_exclusions_),\n",
    "            'celestrak': lambda date_start_, date_end_, date_exclusions_: CelestrakDataset(file_name=celestrak_file, delta_minutes=15, date_start=date_start_, date_end=date_end_, normalize=True, date_exclusions=date_exclusions_),\n",
    "            'solar_inds': lambda date_start_, date_end_, date_exclusions_: SolarIndexDataset(file_name=solar_index_file, delta_minutes=15, date_start=date_start_, date_end=date_end_, normalize=True, date_exclusions=date_exclusions_)\n",
    "        }\n",
    "\n",
    "                    \n",
    "        datasets_jpld_valid = []\n",
    "\n",
    "        date_exclusions = []\n",
    "        aux_datasets_valid_dict = {}\n",
    "\n",
    "        if args.test_event_id:\n",
    "            for event_id in args.test_event_id:\n",
    "                print('Excluding event ID: {}'.format(event_id))\n",
    "                if event_id not in EventCatalog:\n",
    "                    raise ValueError('Event ID {} not found in EventCatalog'.format(event_id))\n",
    "                _, _, exclusion_start, exclusion_end, _, _ = EventCatalog[event_id]\n",
    "                exclusion_start = datetime.datetime.fromisoformat(exclusion_start)\n",
    "                exclusion_end = datetime.datetime.fromisoformat(exclusion_end)\n",
    "                date_exclusions.append((exclusion_start, exclusion_end))\n",
    "\n",
    "                datasets_jpld_valid.append(JPLD(dataset_jpld_dir, date_start=exclusion_start, date_end=exclusion_end))\n",
    "                for name in args.aux_datasets:\n",
    "                    if aux_datasets_valid_dict.get(name) is None:\n",
    "                        aux_datasets_valid_dict[name] = []\n",
    "                    aux_datasets_valid_dict[name].append(dataset_constructors[name](date_start_=exclusion_start, date_end_=exclusion_end, date_exclusions_ = None))\n",
    "\n",
    "        dataset_jpld_valid = UnionDataset(datasets=datasets_jpld_valid)\n",
    "        sum_ = 0\n",
    "        for (st_, end_) in date_exclusions:\n",
    "            dt_ = end_ - st_\n",
    "            n_samples_ = (dt_.days * 24 * 60 // 15 + dt_.seconds // (60 * 15))\n",
    "            sum_ += n_samples_\n",
    "            print(\"expected n_samples: \", n_samples_)\n",
    "        print(sum_)\n",
    "\n",
    "        print(date_exclusions)\n",
    "        print(len(dataset_jpld_valid))\n",
    "        print([(n, len(ds_valid), ds_valid) for n, ds_valid in aux_datasets_valid_dict.items()])\n",
    "        aux_datasets_valid = []\n",
    "        for name, dataset_list in aux_datasets_valid_dict.items():\n",
    "            aux_datasets_valid.append(UnionDataset(datasets=dataset_list)) # NOTE: the union datasets no longer have the same start dates.\n",
    "            print(\"\\nStart and end dates: \", aux_datasets_valid[-1].date_start, aux_datasets_valid[-1].date_end)\n",
    "        # raise\n",
    "        if args.model_type == 'VAE1':\n",
    "            dataset_jpld_train = JPLD(dataset_jpld_dir, date_start=date_start, date_end=date_end, date_exclusions=date_exclusions)\n",
    "            aux_datasets_train = [dataset_constructors[name](date_start_=date_start, date_end_=date_end, date_exclusions_=date_exclusions) for name in args.aux_datasets]\n",
    "\n",
    "            # dataset_train = CompositeDataset([dataset_jpld_train] + aux_datasets_train)\n",
    "            # dataset_valid = CompositeDataset([dataset_jpld_valid] + aux_datasets_valid)\n",
    "            dataset_train = Sequences([dataset_jpld_train] + aux_datasets_train, delta_minutes=args.delta_minutes, sequence_length=1)\n",
    "            dataset_valid = Sequences([dataset_jpld_valid] + aux_datasets_valid, delta_minutes=args.delta_minutes, sequence_length=1)\n",
    "            print(len(dataset_train), len(dataset_valid))\n",
    "            # raise\n",
    "            # dataset_train = dataset_jpld_train\n",
    "            # dataset_valid = dataset_jpld_valid\n",
    "        elif args.model_type == 'IonCastConvLSTM':\n",
    "            dataset_jpld_train = JPLD(dataset_jpld_dir, date_start=date_start, date_end=date_end, date_exclusions=date_exclusions)\n",
    "            aux_datasets_train = [dataset_constructors[name](date_start_=date_start, date_end_=date_end, date_exclusions_=date_exclusions) for name in args.aux_datasets]\n",
    "            dataset_train = Sequences([dataset_jpld_train] + aux_datasets_train, delta_minutes=args.delta_minutes, sequence_length=training_sequence_length)\n",
    "            dataset_valid = Sequences([dataset_jpld_valid] + aux_datasets_valid, delta_minutes=args.delta_minutes, sequence_length=training_sequence_length)\n",
    "        else:\n",
    "            raise ValueError('Unknown model type: {}'.format(args.model_type))\n",
    "\n",
    "        print(aux_datasets_train)\n",
    "        c_dim = sum([ds[0][0].shape[-1] for ds in aux_datasets_train])\n",
    "        print(f'conditioning dimension: {c_dim}')\n",
    "\n",
    "        print('\\nTrain size: {:,}'.format(len(dataset_train)))\n",
    "        print('Valid size: {:,}'.format(len(dataset_valid)))\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            dataset_train, \n",
    "            batch_size=args.batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=args.num_workers,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "            prefetch_factor=4,\n",
    "        ) # sample dims for imgs    : [batch_size, seq_len, ch, h, w]\n",
    "            # sample dims for tabular : [batch_size, seq_len, n_feats]\n",
    "\n",
    "        valid_loader = DataLoader(dataset_valid, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
    "        print(len(valid_loader), len(dataset_valid))\n",
    "\n",
    "    elif args.mode == 'test':\n",
    "        raise NotImplementedError(\"Testing mode is not implemented yet.\")\n",
    "\n",
    "    end_time = datetime.datetime.now()\n",
    "    print('End time: {}'.format(end_time))\n",
    "    print('Total duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62aee336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from model file: ./train-conditioned-2/epoch-02-model.pth\n",
      "Next epoch    : 3\n",
      "Next iteration: 20,172\n",
      "\n",
      "Number of parameters: 13,839,587\n",
      "\n",
      "\n",
      "*** Epoch 3/100 started\n",
      "*** Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 1106016, 1106020, 1106021, 1106022) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEmpty\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ioncast_halil/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1284\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1283\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1284\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ioncast_halil/lib/python3.13/queue.py:212\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    213\u001b[39m \u001b[38;5;28mself\u001b[39m.not_empty.wait(remaining)\n",
      "\u001b[31mEmpty\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     38\u001b[39m model.train()\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total=\u001b[38;5;28mlen\u001b[39m(train_loader)) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     41\u001b[39m         \u001b[38;5;66;03m# a = 1/0\u001b[39;00m\n\u001b[32m     42\u001b[39m         optimizer.zero_grad()\n\u001b[32m     44\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m args.model_type == \u001b[33m'\u001b[39m\u001b[33mVAE1\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     45\u001b[39m             \u001b[38;5;66;03m# jpld_dataset, omni_dataset, celestrak_dataset, solar_index_dataset\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ioncast_halil/lib/python3.13/site-packages/torch/utils/data/dataloader.py:490\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28mself\u001b[39m._iterator = \u001b[38;5;28mself\u001b[39m._get_iterator()\n\u001b[32m    489\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterator\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    491\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ioncast_halil/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1263\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._reset\u001b[39m\u001b[34m(self, loader, first_iter)\u001b[39m\n\u001b[32m   1261\u001b[39m resume_iteration_cnt = \u001b[38;5;28mself\u001b[39m._num_workers\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m resume_iteration_cnt > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m     return_idx, return_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1264\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_idx, _utils.worker._ResumeIteration):\n\u001b[32m   1265\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m return_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ioncast_halil/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1443\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1441\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1442\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1443\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1444\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1445\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ioncast_halil/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1297\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1295\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) > \u001b[32m0\u001b[39m:\n\u001b[32m   1296\u001b[39m     pids_str = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(w.pid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1298\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) exited unexpectedly\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1299\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1300\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue.Empty):\n\u001b[32m   1301\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: DataLoader worker (pid(s) 1106016, 1106020, 1106021, 1106022) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "with Tee(log_file):\n",
    "    if args.mode == 'train':\n",
    "        # check if a previous training run exists in the target directory, if so, find the latest model file saved, resume training from there by loading the model instead of creating a new one\n",
    "        model_files = glob.glob('{}/epoch-*-model.pth'.format(args.target_dir))\n",
    "        if len(model_files) > 0:\n",
    "            model_files.sort()\n",
    "            model_file = model_files[-1]\n",
    "            print('Resuming training from model file: {}'.format(model_file))\n",
    "            model, optimizer, epoch, iteration, train_losses, valid_losses = load_model(model_file, device)\n",
    "            epoch_start = epoch + 1\n",
    "            iteration = iteration + 1\n",
    "            print('Next epoch    : {:,}'.format(epoch_start+1))\n",
    "            print('Next iteration: {:,}'.format(iteration+1))\n",
    "        else:\n",
    "            print('Creating new model')\n",
    "            if args.model_type == 'VAE1':\n",
    "                model = VAE1(z_dim=512, c_dim=c_dim, sigma_vae=False)\n",
    "            elif args.model_type == 'IonCastConvLSTM':\n",
    "                model = IonCastConvLSTM(input_channels=1, output_channels=1, context_window=args.context_window, prediction_window=args.prediction_window)\n",
    "            else:\n",
    "                raise ValueError('Unknown model type: {}'.format(args.model_type))\n",
    "\n",
    "            optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "            iteration = 0\n",
    "            epoch_start = 0\n",
    "            train_losses = []\n",
    "            valid_losses = []\n",
    "\n",
    "            model = model.to(device)\n",
    "\n",
    "        num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print('\\nNumber of parameters: {:,}\\n'.format(num_params))\n",
    "        \n",
    "        for epoch in range(epoch_start, args.epochs):\n",
    "            print('\\n*** Epoch {:,}/{:,} started'.format(epoch+1, args.epochs))\n",
    "            print('*** Training')\n",
    "            # Training\n",
    "            model.train()\n",
    "            with tqdm(total=len(train_loader)) as pbar:\n",
    "                for i, batch in enumerate(train_loader):\n",
    "                    # a = 1/0\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    if args.model_type == 'VAE1':\n",
    "                        # jpld_dataset, omni_dataset, celestrak_dataset, solar_index_dataset\n",
    "                        jpld, omni, celestrak, solar_idx, ts = batch\n",
    "                        all_indeces = torch.cat((omni, celestrak, solar_idx), dim=-1)\n",
    "        \n",
    "                        # [batch_size, seq_len, n_ch, h, w]\n",
    "                        jpld = jpld[:, 0, :, :, :].float() # take the first frame in sequence (sequence should be len one for the VAE model anyways)\n",
    "                        # [batch_size, seq_len, n_feats]\n",
    "                        all_indeces = all_indeces[:, 0, :].float() # take the first frame in sequence (sequence should be len one for the VAE model anyways)\n",
    "                        jpld = jpld.to(device)\n",
    "                        all_indeces = all_indeces.to(device)\n",
    "\n",
    "                        loss = model.loss(x=jpld, c=all_indeces)\n",
    "                    elif args.model_type == 'IonCastConvLSTM':\n",
    "                        jpld_seq, _ = batch\n",
    "                        jpld_seq = jpld_seq.to(device)\n",
    "                        \n",
    "                        loss = model.loss(jpld_seq, context_window=args.context_window)\n",
    "                    else:\n",
    "                        raise ValueError('Unknown model type: {}'.format(args.model_type))\n",
    "                    \n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    iteration += 1\n",
    "\n",
    "                    train_losses.append((iteration, float(loss)))\n",
    "                    pbar.set_description(f'Epoch {epoch + 1}/{args.epochs}, Loss: {loss.item():.4f}')\n",
    "                    pbar.update(1)\n",
    "\n",
    "            # Validation\n",
    "            print('*** Validation')\n",
    "            model.eval()\n",
    "            valid_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batch in valid_loader:\n",
    "                    if args.model_type == 'VAE1':\n",
    "                        jpld, omni, celestrak, solar_idx, ts = batch\n",
    "                        all_indeces = torch.cat((omni, celestrak, solar_idx), dim=-1)\n",
    "                        # print(jpld.shape, omni.shape, celestrak.shape, solar_idx.shape)\n",
    "        \n",
    "                        jpld = jpld[:, 0, :, :, :].float() # take the first frame in sequence (sequence should be len one for the VAE model anyways)\n",
    "                        all_indeces = all_indeces[:, 0, :].float() # take the first frame in sequence (sequence should be len one for the VAE model anyways)\n",
    "                        jpld = jpld.to(device)\n",
    "                        all_indeces = all_indeces.to(device)\n",
    "                        loss = model.loss(jpld, c=all_indeces)\n",
    "                    elif args.model_type == 'IonCastConvLSTM':\n",
    "                        jpld_seq, _ = batch\n",
    "                        jpld_seq = jpld_seq.to(device)\n",
    "                        loss = model.loss(jpld_seq, context_window=args.context_window)\n",
    "                    else:\n",
    "                        raise ValueError('Unknown model type: {}'.format(args.model_type))\n",
    "                    valid_loss += loss.item()\n",
    "            valid_loss /= len(valid_loader)\n",
    "            valid_losses.append((iteration, valid_loss))\n",
    "            print(f'Validation Loss: {valid_loss:.4f}')\n",
    "\n",
    "            file_name_prefix = f'epoch-{epoch + 1:02d}-'\n",
    "\n",
    "            # Save model\n",
    "            model_file = os.path.join(args.target_dir, f'{file_name_prefix}model.pth')\n",
    "            save_model(model, optimizer, epoch, iteration, train_losses, valid_losses, model_file)\n",
    "\n",
    "            # Plot losses\n",
    "            plot_file = os.path.join(args.target_dir, f'{file_name_prefix}loss.pdf')\n",
    "            print(f'Saving plot to {plot_file}')\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(*zip(*train_losses), label='Training')\n",
    "            plt.plot(*zip(*valid_losses), label='Validation')\n",
    "            plt.xlabel('Iteration')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.yscale('log')\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            plt.savefig(plot_file)\n",
    "            plt.close()\n",
    "\n",
    "            # Plot model eval results\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                num_evals = args.num_evals\n",
    "\n",
    "                if args.model_type == 'VAE1':\n",
    "                    # Set random seed for reproducibility of evaluation samples across epochs\n",
    "                    rng_state = torch.get_rng_state()\n",
    "                    torch.manual_seed(args.seed)\n",
    "\n",
    "                    # Reconstruct a batch from the validation set\n",
    "                    # jpld_orig, omni, celestrak, solar_idx, jpld_orig_dates  = next(iter(valid_loader)) \n",
    "                    print(len(dataset_valid))\n",
    "                    print(len(valid_loader))\n",
    "                    for b in valid_loader:\n",
    "                        print(b)\n",
    "                        for t in b: \n",
    "                            try:\n",
    "                                print(t.shape)\n",
    "                            except:\n",
    "                                print(t)\n",
    "                        pass\n",
    "                    batch = next(iter(valid_loader))\n",
    "                    jpld_orig, tabular_datasets_orig, jpld_orig_dates = batch[0], batch[1:-1], batch[-1][0] # dates stored as a list continaing a single tuple\n",
    "                    print(f\"jpld orig dates: {jpld_orig_dates}\")\n",
    "\n",
    "                    all_indeces = torch.cat(tabular_datasets_orig, dim=-1)\n",
    "                    print(jpld_orig.shape, all_indeces.shape, jpld_orig_dates)\n",
    "                    jpld_orig = jpld_orig[:num_evals]\n",
    "                    jpld_orig_dates = jpld_orig_dates[:num_evals]\n",
    "                    all_indeces = all_indeces[:num_evals]\n",
    "                    print(jpld_orig.shape, all_indeces.shape)\n",
    "                    jpld_orig = jpld_orig[:, 0, :, :, :].float() # take the first frame in sequence (sequence should be len one for the VAE model anyways)\n",
    "                    all_indeces = all_indeces[:, 0, :].float() # take the first frame in sequence (sequence should be len one for the VAE model anyways)\n",
    "                    jpld_orig = jpld_orig.to(device)\n",
    "                    all_indeces = all_indeces.to(device)\n",
    "                    jpld_recon, _, _ = model.forward(jpld_orig, all_indeces)\n",
    "                    jpld_orig_unnormalized = JPLD.unnormalize(jpld_orig)\n",
    "                    jpld_recon_unnormalized = JPLD.unnormalize(jpld_recon)\n",
    "                    print(f\"jpld_recon_unnormalized shape: {jpld_recon_unnormalized.shape}\")\n",
    "\n",
    "                    # Sample a batch from the model\n",
    "                    jpld_sample = model.sample(n=num_evals, c=all_indeces)\n",
    "                    print(f\"jpld_samp shape: {jpld_sample.shape}\")\n",
    "                    jpld_sample_unnormalized = JPLD.unnormalize(jpld_sample)\n",
    "                    jpld_sample_unnormalized = jpld_sample_unnormalized.clamp(0, 100)\n",
    "                    torch.set_rng_state(rng_state)\n",
    "                    # Resume with the original random state\n",
    "\n",
    "                    # Save plots\n",
    "                    for i in range(num_evals):\n",
    "                        date = jpld_orig_dates[i]\n",
    "                        print(type(date), date, jpld_orig_dates)\n",
    "                        if isinstance(date, str):\n",
    "                            date_str = datetime.datetime.fromisoformat(date).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                        elif isinstance(date, datetime.datetime):\n",
    "                            date_str = date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                        recon_original_file = os.path.join(args.target_dir, f'{file_name_prefix}reconstruction-original-{i+1:02d}.pdf')\n",
    "                        save_gim_plot(jpld_orig_unnormalized[i][0].cpu().numpy(), recon_original_file, vmin=0, vmax=100, title=f'JPLD GIM TEC, {date_str}')\n",
    "\n",
    "                        recon_file = os.path.join(args.target_dir, f'{file_name_prefix}reconstruction-{i+1:02d}.pdf')\n",
    "                        save_gim_plot(jpld_recon_unnormalized[i][0].cpu().numpy(), recon_file, vmin=0, vmax=100, title=f'JPLD GIM TEC, {date_str} (Reconstruction)')\n",
    "\n",
    "                        sample_file = os.path.join(args.target_dir, f'{file_name_prefix}sample-{i+1:02d}.pdf')\n",
    "                        save_gim_plot(jpld_sample_unnormalized[i][0].cpu().numpy(), sample_file, vmin=0, vmax=100, title='JPLD GIM TEC (Sampled from model)')\n",
    "\n",
    "\n",
    "                elif args.model_type == 'IonCastConvLSTM':\n",
    "                    # jpld_seq, dates_seq = next(iter(valid_loader))\n",
    "                    # jpld_seq = jpld_seq[:num_evals]\n",
    "                    # dates_seq = [dates_seq[t][:num_evals] for t in range(args.context_window + args.prediction_window)]\n",
    "                    # jpld_seq = jpld_seq.to(device)\n",
    "\n",
    "                    # jpld_contexts = jpld_seq[:, :args.context_window, :, :]\n",
    "                    # jpld_forecasts_originals = jpld_seq[:, args.context_window:, :, :]\n",
    "\n",
    "                    # # Forecasts\n",
    "                    # jpld_forecasts = model.predict(jpld_contexts, prediction_window=args.prediction_window)\n",
    "\n",
    "                    # jpld_contexts_unnormalized = JPLD.unnormalize(jpld_contexts)\n",
    "                    # jpld_forecasts_unnormalized = JPLD.unnormalize(jpld_forecasts)\n",
    "                    # jpld_forecasts_originals_unnormalized = JPLD.unnormalize(jpld_forecasts_originals)\n",
    "\n",
    "                    # # save forecasts\n",
    "\n",
    "                    # # # jpld_seq_dates is a nested list of dates with shape (context_window + prediction_window, batch_size)\n",
    "                    # # for b in range(args.batch_size):\n",
    "                    # #     print(f'Batch {b+1}/{args.batch_size} dates:')\n",
    "                    # #     for t in range(args.context_window + args.prediction_window):\n",
    "                    # #         print(dates_seq[t][b])\n",
    "                    # for i in range(num_evals):\n",
    "                    #     dates = [dates_seq[t][i] for t in range(args.context_window + args.prediction_window)]\n",
    "                    #     dates_context = [datetime.datetime.fromisoformat(d).strftime('%Y-%m-%d %H:%M:%S') for d in dates[:args.context_window]]\n",
    "                    #     dates_forecast = [datetime.datetime.fromisoformat(d).strftime('%Y-%m-%d %H:%M:%S') for d in dates[args.context_window:args.context_window + args.prediction_window]]\n",
    "                    #     dates_forecast_ahead = ['{} mins'.format((j + 1) * 15) for j in range(args.prediction_window)]\n",
    "                    #     # save videos of the forecasts\n",
    "                    #     forecast_video_file = os.path.join(args.target_dir, f'{file_name_prefix}forecast-{i+1:02d}.mp4')\n",
    "                    #     save_gim_video(\n",
    "                    #         jpld_forecasts_unnormalized.cpu().numpy()[i].reshape(args.prediction_window, 180, 360),\n",
    "                    #         forecast_video_file,\n",
    "                    #         vmin=0, vmax=100,\n",
    "                    #         titles=[f'JPLD GIM TEC Forecast: {d} ({mins_ahead})' for d, mins_ahead in zip(dates_forecast, dates_forecast_ahead)]\n",
    "                    #     )\n",
    "\n",
    "                    #     # save comparison video (original vs forecast)\n",
    "                    #     comparison_video_file = os.path.join(args.target_dir, f'{file_name_prefix}forecast-comparison-{i+1:02d}.mp4')\n",
    "                    #     save_gim_video_comparison(\n",
    "                    #         jpld_forecasts_originals_unnormalized.cpu().numpy()[i].reshape(args.prediction_window, 180, 360),  # top (original)\n",
    "                    #         jpld_forecasts_unnormalized.cpu().numpy()[i].reshape(args.prediction_window, 180, 360),  # bottom (forecast)\n",
    "                    #         comparison_video_file,\n",
    "                    #         vmin=0, vmax=100,\n",
    "                    #         titles_top=[f'JPLD GIM TEC Original: {d}' for d in dates_forecast],\n",
    "                    #         titles_bottom=[f'JPLD GIM TEC Forecast: {d} ({mins_ahead})' for d, mins_ahead in zip(dates_forecast, dates_forecast_ahead)]\n",
    "                    #     )\n",
    "\n",
    "                    #     if epoch == 0:\n",
    "                    #         # save videos of the forecasts originals\n",
    "                    #         forecast_original_video_file = os.path.join(args.target_dir, f'{file_name_prefix}forecast-original-{i+1:02d}.mp4')\n",
    "                    #         save_gim_video(\n",
    "                    #             jpld_forecasts_originals_unnormalized.cpu().numpy()[i].reshape(args.prediction_window, 180, 360),\n",
    "                    #             forecast_original_video_file,\n",
    "                    #             vmin=0, vmax=100,\n",
    "                    #             titles=[f'JPLD GIM TEC: {d}' for d in dates_forecast]\n",
    "                    #         )\n",
    "\n",
    "                    #         # save videos of the contexts\n",
    "                    #         context_video_file = os.path.join(args.target_dir, f'{file_name_prefix}context-{i+1:02d}.mp4')\n",
    "                    #         save_gim_video(\n",
    "                    #             jpld_contexts_unnormalized.cpu().numpy()[i].reshape(args.context_window, 180, 360),\n",
    "                    #             context_video_file,\n",
    "                    #             vmin=0, vmax=100,\n",
    "                    #             titles=[f'JPLD GIM TEC: {d}' for d in dates_context]\n",
    "                    #         )\n",
    "\n",
    "                    if args.test_event_id:\n",
    "                        for event_id in args.test_event_id:\n",
    "                            if event_id not in EventCatalog:\n",
    "                                raise ValueError('Event ID {} not found in EventCatalog'.format(event_id))\n",
    "                            event = EventCatalog[event_id]\n",
    "                            _, _, date_start, date_end, _, max_kp = event\n",
    "                            print('Testing event ID: {}'.format(event_id))\n",
    "                            date_start = datetime.datetime.fromisoformat(date_start)\n",
    "                            date_end = datetime.datetime.fromisoformat(date_end)\n",
    "                            date_forecast_start = date_start + datetime.timedelta(minutes=model.context_window * args.delta_minutes)\n",
    "                            file_name = f'{file_name_prefix}test-event-{event_id}-kp{max_kp}-{date_start.strftime(\"%Y%m%d%H%M\")}-{date_end.strftime(\"%Y%m%d%H%M\")}.mp4'\n",
    "                            title = f'Event: {event_id}, Kp={max_kp}'\n",
    "                            run_forecast(model, dataset_valid, date_start, date_end, date_forecast_start, title, file_name, args)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ioncast_halil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
